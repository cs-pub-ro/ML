{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfIdnkh92358"
   },
   "source": [
    "# Lab 8 - RNN\n",
    "\n",
    "This is a tutorial training various RNNs on simple datasets and doing some analysis.\n",
    "In acest laborator vom parcurge notiuni introductive privind retelele recurente si folosirea lor pentru a prezice valori viitoare ale unei serii (i.e. setup predictiv - de regresie).\n",
    "\n",
    "Structura:\n",
    "  1. Implementarea unei celule RNN simple\n",
    "  2. Implementarea unui model folosind o celula LSTM\n",
    "  3. Intelegerea conceptului de **teacher forcing** si **warm start** (in cadrul predictiei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA_K3_OL3EY4"
   },
   "source": [
    "## Importuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "j5YGV2hb2RIt"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import LSTM\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0KPZdiq5AVJ"
   },
   "source": [
    "# Task 1.    Vanilla RNN\n",
    "\n",
    "Implementati o celula RNN de baza folosind straturi liniari din PyTorch.\n",
    "\n",
    "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "   \n",
    "   Unde\n",
    "   \n",
    "   * $x_t$ inputul la momentul $t$ de dimensiunea (batch_size, input_features)\n",
    "   * $h_t$ hidden state at time $t$ de dimensiunea (batch_size, hidden_size)\n",
    "   * $W$ proiectie input-to-hidden (antrenabil) \n",
    "   * $V$ proiectie hidden-to-hidden (antrenabil)\n",
    "   * $b$ bias (antrenabil)\n",
    "   * $f$ functia non-lineara aleasa (de regula tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "z6GIHgOnzd8Y"
   },
   "outputs": [],
   "source": [
    "#@title Vanilla RNN\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False, activation=nn.Tanh):\n",
    "        \"\"\"\n",
    "        Creates a vanilla RNN where input-to-hidden is a nn.Linear layer\n",
    "        and hidden-to-output is a nn.Linear layer\n",
    "        \n",
    "        :param input_size: the size of the input to the RNN\n",
    "        :param hidden_size: size of the hidden state of the RNN\n",
    "        :param output_size: size of the output\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        self._activation = activation()\n",
    "        \n",
    "        self.in_to_hidden = nn.Linear(self._input_size, self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_hidden = nn.Linear(self._hidden_size, self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_out = nn.Linear(self._hidden_size, self._output_size, bias=self._bias)\n",
    "    \n",
    "    def step(self, input, hidden=None):\n",
    "        ## TODO 1.1:\n",
    "        ## Proiectați intrarea la dimensiunea stratului ascuns;  \n",
    "        ## Obțineți vectorul ascuns actualizat - h_t = f_activare(W_ih x input + W_hh x h_{t-1})\n",
    "        ## Obțineți ieșirea prin proiectarea vectorului ascuns actualizat la dimensiunea stratului de ieșire.  \n",
    "        \n",
    "        # Valori default de tensori 0 -- A SE INLOCUI\n",
    "        hidden_ = torch.zeros(self._hidden_size, requires_grad=True).to(device)\n",
    "        output_ = torch.zeros(self._output_size, requires_grad=True).to(device)\n",
    "        \n",
    "        return output_, hidden_\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, hidden=None, teacher_forcing_prob=0.5, warm_start=10):\n",
    "        batch_size = inputs.size(0)\n",
    "        steps = inputs.size(1)\n",
    "        \n",
    "        outputs = torch.autograd.Variable(torch.zeros(batch_size, steps, self._output_size)).to(device)\n",
    "        \n",
    "        output_ = None\n",
    "        hidden_ = hidden\n",
    "        \n",
    "        ## TODO 1.2 - your code here: propagati secventa de antrenare prin retea\n",
    "        ## Nota: \n",
    "        ##    inputs - tensor de forma (batch_size, seq_len, 1)\n",
    "        ##    self.step primeste intrare de forma input: (batch_size, 1), hidden: (batch_size, hidden_size)\n",
    "        ##    self.step intoarche iesire de forma output: (batch_size, 1), hidden: (batch_size, hidden_size)\n",
    "        for i in range(steps):\n",
    "            # Definiti forma lui input_ de trimis catre self.step() \n",
    "            # Valoare default = tensor cu valori de 0 - A SE INLOCUI\n",
    "            input_ = torch.zeros(batch_size, self._input_size).to(device)\n",
    "            # Dacă este primul pas (i=0), există o perioadă de warm_start (context)  \n",
    "            # în timpul testării sau se aplică teacher forcing, \n",
    "            # atunci următoarea input_ ar trebui selectată din valorile reale (ground truth).  \n",
    "            # În caz contrar, input_ ar trebui să fie ceea ce rețeaua a returnat  \n",
    "            # la pasul anterior de timp. \n",
    "            # input_ = ...\n",
    "            \n",
    "            # calculam un pas de recurenta\n",
    "            output_, hidden_ = self.step(input_, hidden_)\n",
    "            \n",
    "            # scriem iesirea output_ de forma (batch_size, 1) la indexul i (pe dimensiunea de lungime a secventei)\n",
    "            # in tensorul de iesire outputs\n",
    "            outputs[:, i, :] = output_\n",
    "        \n",
    "        return outputs, hidden_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.    RNN simplu folosind un modul LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False, batch_first = True, num_layers = 1):\n",
    "        super(LSTMRNN, self).__init__()\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.rnn = LSTM(input_size = input_size, \n",
    "                        proj_size = output_size, \n",
    "                        hidden_size = hidden_size, \n",
    "                        bias=bias,\n",
    "                        num_layers = num_layers, \n",
    "                        batch_first = batch_first)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, hidden=None, teacher_forcing_prob=0.5, warm_start=10):\n",
    "        batch_size = inputs.size(0)\n",
    "        steps = inputs.size(1)\n",
    "\n",
    "        outputs = torch.autograd.Variable(torch.zeros(batch_size, steps, self._output_size)).to(device)\n",
    "        \n",
    "        output_ = torch.zeros(batch_size, 1, self._output_size).to(device)\n",
    "        hidden_ = None\n",
    "\n",
    "        for i in range(steps):\n",
    "            ## TODO 2.1: propagati secventa de antrenare prin retea\n",
    "            ## Nota: \n",
    "            ##    inputs - tensor de forma (batch_size, seq_len, 1)\n",
    "            ##    self.rnn primeste intrare de forma \n",
    "            ##      input_: (batch_size, seq_len=1, input_size=1), \n",
    "            ##      hidden_: tuplu de (hidden_state, cell_state) de dim. (num_layers, batch_size, hidden_size)\n",
    "            ##    self.rnn intoarche iesire de forma output: (batch_size, seq_len=1, output_size=1)\n",
    "            \n",
    "            # Definiti forma lui input_ de trimis catre self.step() \n",
    "            # Valoare default = tensor cu valori de 0 - A SE INLOCUI\n",
    "            input_ = torch.zeros(batch_size, 1, self._input_size).to(device)\n",
    "            \n",
    "            # Dacă este primul pas (i=0), există o perioadă de warm_start (context)  \n",
    "            # în timpul testării sau se aplică teacher forcing, \n",
    "            # atunci următoarea input_ ar trebui selectată din valorile reale (ground truth).  \n",
    "            # În caz contrar, input_ ar trebui să fie ceea ce rețeaua a returnat  \n",
    "            # la pasul anterior de timp. \n",
    "            ## input_ = ...\n",
    "            \n",
    "            ## end code\n",
    "            output_, hidden_ = self.rnn(input_, hidden_)\n",
    "            outputs[:, i, :] = output_.squeeze(1)\n",
    "        \n",
    "        return outputs, hidden_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antrenarea retelei RNN pentru a prezice o curba sinusoidala\n",
    "\n",
    "Antrenam reteaua pe o curba sinusoidala - predictia urmatoarei valori din sinusoida pe baza valorilor *prezise* anterior.\n",
    "\n",
    "Predictie pentru   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
    "\n",
    "În special, dorim ca rețeaua să prezică următoarea valoare într-o buclă, condiționând predicția de unele valori inițiale (furnizate) și de toate predicțiile ulterioare.\n",
    "\n",
    "Pentru a învăța modelul de predicție, vom folosi **teacher forcing**. Aceasta înseamnă că atunci când antrenam modelul, intrarea la momentul $t$ este secvența reală la momentul $t$, mai degrabă decât rezultatul produs de model la $t-1$. \n",
    "\n",
    "**Când dorim să generăm** date din model, **nu avem acces la succesiunea adevărată**, așa că nu folosim *teacher forcing*. \n",
    "Totuși, în cazul problemei noastre, vom folosi notiunea de **warm start (pornire la cald)**, deoarece avem nevoie de mai mulți pași de timp pentru a prezice următoarea valoare a undei sinusoidale (cel puțin 2, pentru valoarea inițială și pentru pas).\n",
    "\n",
    "Codul de mai jos derulează modelul RNN pe care l-ați definit mai sus, face antrenamentul folosind backprop în timp (BPTT) și trasează datele reale (\"ground truth\"), datele generate în timpul antrenamentului (\"training predictions\") și eșantioanele de model \"generate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code @ cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running code @ {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "WARM_START = 8  #@param {type:\"integer\"}\n",
    "TEACHER_FORCING_PROB = 0.25  #@param {type:\"slider\", min:0.0, max:1.0, step:0.25}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.001  #@param {type:\"number\"}\n",
    "REPORTING_INTERVAL = 200  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
    "\n",
    "# We create training data, sine wave over [0, 2pi]\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1)\n",
    "y_train = np.sin(x_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(y_train, unroll_length, batch_size):\n",
    "    y_batch = np.zeros((batch_size, unroll_length, 1))\n",
    "    starts = np.zeros(batch_size, dtype=np.int64)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        start = np.random.choice(range(y_train.shape[0] - UNROLL_LENGTH))\n",
    "        train_sequence = y_train[start : (start + UNROLL_LENGTH)]\n",
    "        \n",
    "        starts[i] = start\n",
    "        y_batch[i, :, :] = train_sequence\n",
    "        \n",
    "    return y_batch, starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = VanillaRNN(hidden_size=HIDDEN_UNITS, bias=True)\n",
    "# net = LSTMRNN(hidden_size=HIDDEN_UNITS, bias=True, num_layers = 1)\n",
    "net = net.to(device)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # select a start point in the training set for a sequence of UNROLL_LENGTH\n",
    "    train_batch, starts = create_batch(y_train, UNROLL_LENGTH, BATCH_SIZE)\n",
    "    \n",
    "    train_inputs = torch.from_numpy(train_batch[:,:-1,:]).float().view(BATCH_SIZE, UNROLL_LENGTH-1, 1).to(device)\n",
    "    train_targets = torch.from_numpy(train_batch[:,1:, :]).float().view(BATCH_SIZE, UNROLL_LENGTH-1, 1).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs, hidden = net(train_inputs, \n",
    "                          hidden=None, \n",
    "                          teacher_forcing_prob=TEACHER_FORCING_PROB, \n",
    "                          warm_start=0)\n",
    "    \n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % REPORTING_INTERVAL == REPORTING_INTERVAL - 1:\n",
    "        # let's see how well we do on predictions for the whole sequence\n",
    "        avg_loss = running_loss / REPORTING_INTERVAL\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Select a start point within the first 75% of y_train\n",
    "        max_start_index = int(0.75 * len(y_train))\n",
    "        report_start_index = np.random.choice(range(max_start_index))\n",
    "        \n",
    "        # Create the reporting sequence\n",
    "        report_sequence = torch.from_numpy(y_train[report_start_index:-1]).float().unsqueeze(0).to(device)\n",
    "        report_targets = torch.from_numpy(y_train[report_start_index + 1:]).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        net.eval()\n",
    "        report_output, report_hidden = net(report_sequence, hidden=None, \n",
    "                                           teacher_forcing_prob=0.0, \n",
    "                                           warm_start=WARM_START)\n",
    "        \n",
    "        report_loss = criterion(report_output, report_targets)\n",
    "        print('[%d] avg_loss: %.5f, report_loss: %.5f, ' % (iteration + 1, avg_loss, report_loss.item()))\n",
    "        # print(outputs[0, :, :].detach().data.cpu().numpy().ravel())\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Training Loss %.5f;  Sampling loss %.5f; Iteration %d' % (avg_loss, report_loss.item(), iteration))\n",
    "        \n",
    "        plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
    "               linestyle=\":\", lw=6)\n",
    "        plt.plot(range(starts[0], starts[0]+UNROLL_LENGTH-1), \n",
    "                 outputs[0, :, :].detach().data.cpu().numpy().ravel(), \n",
    "                 c='gold', label='Train prediction', lw=5, marker=\"o\", markersize=5, alpha=0.7)\n",
    "        plt.plot(range(report_start_index, len(y_train)-1), \n",
    "                 report_output.detach().data.cpu().numpy().ravel(), \n",
    "                 c='r', label='Generated', lw=4, alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jCR9YGaI7my"
   },
   "source": [
    "# TASK 3: Evaluarea influentei hiper-parametrilor\n",
    "\n",
    "Testati pe baza codului de mai sus urmatoarele configuratii:\n",
    "  * BATCH_SIZE: 1, 16 (default), 32\n",
    "  * UNROLL_LENGTH: 6, 30 (default), 62\n",
    "  * TEACHER_FORCING: 0.25 (default), 1.0\n",
    "  \n",
    "Raspundeti la urmatoarele intrebari:\n",
    "  1. Care este influenta TEACHER_FORCING-ului? In ce situatii ar trebui evitat / redus?\n",
    "  2. Care este influenta UNROLL_LENGTH-ului (lungimea secventei de antrenare)? Ce se poate intampla daca UNROLL_LENGTH este foarte mic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwojS8kx0QyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEML2019_RNN_full.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
