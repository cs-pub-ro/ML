{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tema 2 - Skel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/homework/2020-2021/hw2/Tema_2_Skel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLbq2kmY8Pg7"
      },
      "source": [
        "# Tema 2 - Taxi Driver\n",
        "\n",
        "### Autori: \n",
        "* George Muraru\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUqcqknZ8ZzX"
      },
      "source": [
        "## 2. Scopul temei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXJjFpkZ8dM0"
      },
      "source": [
        "Tema are ca scop folosirea unor tehnici simple de reinforcement learning pentru a realiza un bot care \"știe\" să realizeze o sarcină simplă: să transporte o persoană dintr-o locație în alta.\n",
        "\n",
        "Mai multe detalii despre joc se pot găsi [aici](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) și [aici](https://gym.openai.com/envs/Taxi-v3/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8nGcW70-CO6"
      },
      "source": [
        "## 3. Workspace Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoBV0vEQ0vhu"
      },
      "source": [
        "Pentru environmentul de joc, vom folosi \"Gym\" - un tool oferit de OpenAI pentru Reinforcement Learning.\n",
        "\n",
        "Mai multe detalii puteți găsi [aici](https://gym.openai.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4HsagGt-zvI"
      },
      "source": [
        "### Dependețe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILG0Xlj3-1yh"
      },
      "source": [
        "!pip install -U gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsezxQc6Cw2q"
      },
      "source": [
        "### Import biblioteci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRBWl2gNCz0s"
      },
      "source": [
        "import gym\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set seaborn plotting type\n",
        "sns.set()\n",
        "\n",
        "\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koOr44nVC2W8"
      },
      "source": [
        "### Setup Environment pentru Taxi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QqIehW-C6jt"
      },
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIrkE5md-DwS"
      },
      "source": [
        "### Parametrii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq_2kBEe-I-s"
      },
      "source": [
        "LOG_INTERVAL = 100 #@param {type: \"slider\", min: 0, max: 1000, step: 10}\n",
        "NR_EPISODES = 2000 #@param {type: \"slider\", min: 5, max: 10000, step: 5}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7CBF_u4Ifxf"
      },
      "source": [
        "### Hyperparametrii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5soyOMbIg-I"
      },
      "source": [
        "ALPHA = 0.1 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
        "GAMMA = 0.9 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
        "EPSILON = 0.2 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.1}\n",
        "\n",
        "# For BONUS\n",
        "\n",
        "DECAY_EPS = 0.95 #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.05}\n",
        "DECAY_EPS_EPISODES = 100 #@param {type: \"slider\", min: 0, max: 10000, step: 5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR10YUe9BXKn"
      },
      "source": [
        "## 4 Problemă de rezolvat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2jAK1qtDooZ"
      },
      "source": [
        "Dorim să implementăm un bot care utilizează Q-Learning sau Sarsa pentru a realiza un task simplu - transportarea unei persoane de la o poziție la alta.\n",
        "\n",
        "De asemenea, dorim să observăm ce se întamplă când modifăm diferiți parametrii ai sistemului:\n",
        " - dăm un reward mai mic/mare pentru mișcări inutile\n",
        " - scoatem anumite informații din stare\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZ4u0XLKKJ0"
      },
      "source": [
        "### Choose action (5p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlwih38w3z4T"
      },
      "source": [
        "Funcția `choose_action` are ca parametrii:\n",
        "- `Q` - tabela de stări-acțiuni/tabelă de utilităti (aceasta se va modifica în pasul de antrenare),\n",
        "- `state` - starea curentă în care se află jucătorul (în cazul nostru șoferul)\n",
        "- `eps` - probabilitatea cu care se va alege o mutare random, are rolul de a controla tradeoff-ul dintre *Explorare* și *Exploatare* \n",
        "\n",
        "Funcția `choose_action` trebuie să intoarcă o acțiune random cu probabilitate `eps` sau acțiunea care maximizează utilitatea din starea curentă."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmImyaTvKKo-"
      },
      "source": [
        "def choose_action(Q: np.array, state: int, eps: float = 0.0) -> int:\n",
        "    # TODO: Select an action\n",
        "    action = 0\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvilu_zqD4LI"
      },
      "source": [
        "### 4.1 Q-Learning (25p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzpZ49jb9K2b"
      },
      "source": [
        "Implementați algoritmul Q-Learning.\n",
        "\n",
        "Formula de update pentru `Q` este:\n",
        "$$\n",
        "  Q[s_t, a] = Q[s_t, a] + \\alpha(R + \\gamma \\max_{a}Q[s_{t+1}, a] - Q[s_t, a])\n",
        "$$\n",
        "\n",
        "Unde:\n",
        "- `s_t` - starea la momentul `t`\n",
        "- `a` - acțiunea aleasă la momentul `t`\n",
        "- $\\alpha$, $\\gamma$ - parametru setat înainte de rulare\n",
        "- `s_{t+1}` - starea la momentul `t+1`, după ce s-a efectua acțiunea `a`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuQAPbb4EvCC"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfoLA8WSE26f"
      },
      "source": [
        "REWARDS = []\n",
        "eps = EPSILON\n",
        "\n",
        "# Table used to keep the utilities for state-action\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "env.seed(42)\n",
        "\n",
        "\n",
        "for i in range(1, NR_EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    has_ended = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not has_ended:\n",
        "        # TODO: Get the action\n",
        "        # action = ...\n",
        "\n",
        "        # Do the action\n",
        "        next_state, reward, has_ended, _ = env.step(action)\n",
        "\n",
        "        # Cumulate the reward for plotting\n",
        "        total_reward += reward\n",
        "\n",
        "        # TODO: Update the Q(state, action) with the new value\n",
        "        # Q[state][action] = ...\n",
        "\n",
        "        # TODO: Go to the new state\n",
        "        # state = ...\n",
        "\n",
        "    if i % LOG_INTERVAL == 0:\n",
        "        REWARDS.append(total_reward)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "    # TODO (BONUS): Decay eps for every DECAY_EPS_EPISODES\n",
        "    # Use DECAY_EPS\n",
        "\n",
        "# Save the results for plotting\n",
        "Q_1 = Q\n",
        "print(\"Training Q-Learning finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya53mUXoyMki"
      },
      "source": [
        "#### Let's check the driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as7iNa86yQGL"
      },
      "source": [
        "state = env.reset()\n",
        "has_ended = False\n",
        "\n",
        "while not has_ended:\n",
        "    action = choose_action(Q, state)\n",
        "    state, reward, has_ended, _ = env.step(action)\n",
        "    time.sleep(0.75)\n",
        "    env.render()\n",
        "    print(f\"Reward {reward}\")\n",
        "    clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG0vWU5W2y3L"
      },
      "source": [
        "### And the rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfW3gEtA23cY"
      },
      "source": [
        "x_points = np.arange(LOG_INTERVAL, NR_EPISODES + 1, LOG_INTERVAL)\n",
        "y_points = REWARDS\n",
        "\n",
        "plt = sns.lineplot(x=x_points, y=y_points)\n",
        "plt.set(xlabel=\"# episodes\", ylabel=\"reward\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNZkA1ID7HC"
      },
      "source": [
        "### 4.2 SARSA (25p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPJMq4Z_0f-"
      },
      "source": [
        "Implementați algoritmul SARSA.\n",
        "\n",
        "Formula de update pentru `Q` este:\n",
        "$$\n",
        "  Q[s_t, a] = Q[s_t, a] + \\alpha(R + \\gamma Q[s_{t+1}, a'] - Q[s_t, a])\n",
        "$$\n",
        "\n",
        "Unde:\n",
        "- $s_t$ - starea la momentul `t`\n",
        "- **a** - acțiunea aleasă la momentul `t`\n",
        "- **a'** - acțiunea aleasă la momentul `t+1`, presupunând ca suntem în starea $s_{t+1}$\n",
        "- $\\alpha$, $\\gamma$ - parametru setat înainte de rulare\n",
        "- $s_{t+1}$ - starea la momentul `t+1`, după ce s-a efectua acțiunea `a`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afpcVDWGJpvE"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufning8fJrUI"
      },
      "source": [
        "REWARDS = []\n",
        "eps = EPSILON\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "env.seed(42)\n",
        "\n",
        "\n",
        "for i in range(1, NR_EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    has_ended = False\n",
        "    total_reward = 0\n",
        "\n",
        "    # TODO: Get the action\n",
        "    # action = ...\n",
        "\n",
        "    while not has_ended:\n",
        "        # Do the action\n",
        "        next_state, reward, has_ended, _ = env.step(action)\n",
        "\n",
        "        # TODO: Get the next_action from the next_state\n",
        "        # next_action = ...\n",
        "\n",
        "        # Cumulate the reward for plotting\n",
        "        total_reward += reward\n",
        "\n",
        "        # TODO: Update the Q(state, action) with the new value\n",
        "        # Q[state][action] = ...\n",
        "\n",
        "        # TODO: Go to the new state\n",
        "        # state = ...\n",
        "\n",
        "        # TODO: Update the action\n",
        "        # action = ...\n",
        "\n",
        "\n",
        "    if i % LOG_INTERVAL == 0:\n",
        "        REWARDS.append(total_reward)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "    # TODO (BONUS): Decay eps for every DECAY_EPS_EPISODES\n",
        "    # Use DECAY_EPS\n",
        "\n",
        "\n",
        "# Save for plotting\n",
        "\n",
        "Q_2 = Q\n",
        "print(\"Training SARSA finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmxRA5T9LN96"
      },
      "source": [
        "#### Let's check the driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rOjAgJOLQ8Z"
      },
      "source": [
        "state = env.reset()\n",
        "has_ended = False\n",
        "\n",
        "while not has_ended:\n",
        "    action = choose_action(Q, state)\n",
        "    state, reward, has_ended, _ = env.step(action)\n",
        "    time.sleep(0.75)\n",
        "    env.render()\n",
        "    print(f\"Reward {reward}\")\n",
        "    clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx3EMZteLTeZ"
      },
      "source": [
        "#### And the rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKM9oRJHLU0m"
      },
      "source": [
        "x_points = np.arange(LOG_INTERVAL, NR_EPISODES + 1, LOG_INTERVAL)\n",
        "y_points = REWARDS\n",
        "\n",
        "plt = sns.lineplot(x=x_points, y=y_points)\n",
        "plt.set(xlabel=\"# episodes\", ylabel=\"reward\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYMRn_T6D0sp"
      },
      "source": [
        "### 4.3 Results (35p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsPDimBcMAzc"
      },
      "source": [
        "\n",
        "Modificați parametrii algoritmilor (cele de la începutul Notebook-ului), faceți grafice pentru fiecare caz și explicați rezultatele obținute.\n",
        "\n",
        "Unii (și nu numai) dintre paramatrii pe care îi puteți modifica:\n",
        "- numărul de episoade de antrenare\n",
        "- `eps` mai mic/mare\n",
        "- valoarea lui $\\gamma$\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPC3_zM6L6xb"
      },
      "source": [
        "#### 4.4 And something more... (10p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLV1t6tmL-qc"
      },
      "source": [
        "##### A simple move (5p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hpaILBTPHGR"
      },
      "source": [
        "\n",
        "Dorim să vedem importanța reward-ului și a stării.\n",
        "\n",
        "Modificați celula de antrenare pentru una dintre metode astfel încât reward-ul pentru o **mutare simplă** să fie 0, (acum este -1). Refaceți graficul metodei alese și trageți concluziile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWnABGc9PLNN"
      },
      "source": [
        "##### A little less space (5p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4DvqjEO2uz"
      },
      "source": [
        "**Starea** are valori discrete (500 de valori posibile), iar aceasta este encodată folosind metoda d [aici](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py#L128).\n",
        "\n",
        "Rescrieți codul astfel încât starea să fie encodată folosind:\n",
        "$$\n",
        "(((taxi\\_row * 5) + taxi\\_column) * 5 + pass\\_loc) * 2 + destination\n",
        "$$\n",
        "Antrenați unul dintre algoritmi, faceți graficul și explicați rezultatul.\n",
        "\n",
        "**Atenție!** Trebuie să și modificați cum arată Q (forma matricei).\n",
        "\n",
        "Hint: Decodarea stării este [aici](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py#L139)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9FzqwzeGAi3"
      },
      "source": [
        "## 5 BONUS: Some decay on the way (10p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEeZGRXBGSLn"
      },
      "source": [
        "Analizați ce se întâmplă daca scadeți treptat `eps`, pe măsura antrenării. \n",
        "\n",
        "Intuitiv, dorim ca la începutul antrenării agentul să **exploreze** cât mai mult posibil și să atingă un număr mare de stări (în exemplul nostru cu taxiul, nu avem așa multe stări, dar sunt jocuri în care spațiul stărilor poate fi mai mare).\n",
        "\n",
        "De asemenea, pe măsură ce continuăm antrenarea, dorim ca taximetristul să înceapă să **exploateze** mai mult stările cunoscute cu o utilitate mare.\n",
        "\n"
      ]
    }
  ]
}
