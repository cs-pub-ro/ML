{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab7/Laborator_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RiRI3fHjjIw"
   },
   "source": [
    "# Învățare Automată\n",
    "# Învățare prin Recompensă - Algoritmul Q-learning\n",
    "### Autori:\n",
    "* Tudor Berariu - 2016\n",
    "* George Muraru - 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibC7YWzdjnhW"
   },
   "source": [
    "## 1. Scopul laboratorului"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXiq65whj3QO"
   },
   "source": [
    "Scopul laboratorului îl reprezintă ̆întelegerea și implementarea algoritmului Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2LlVwPokURW"
   },
   "source": [
    "# 2. Algoritmul Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtCzyVGxvKxH"
   },
   "source": [
    "![Q-learning](https://github.com/cs-pub-ro/ML/blob/master/lab/lab7/img/q-learning.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iQkSawwqz4m"
   },
   "source": [
    "## 3. Workspace Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGpoVQBdq2-f"
   },
   "source": [
    "### Câteva biblioteci de care vom avea nevoie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oks9emv_sNHi"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from random import choice, random\n",
    "from time import sleep\n",
    "import urllib.request\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJjdU4Afq58O"
   },
   "source": [
    "## Parametrii necesari rulării"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwW9HiQkxUHy"
   },
   "outputs": [],
   "source": [
    "# File to read map from\n",
    "MAP_NAME = \"mini_map\" #@param [\"mini_map\", \"big_map\", \"huge_map\"]\n",
    "\n",
    "# Meta-parameters\n",
    "\n",
    "LEARNING_RATE = 0.1 #@param {type: \"slider\", min: 0.001, max: 1.0, step: 0.01}\n",
    "DISCOUNT_FACTOR = 0.99 #@param {type: \"slider\", min: 0.01, max: 1.0, step: 0.01}\n",
    "\n",
    "# Probabilit to choose a random action\n",
    "EPSILON = 0.05 #@param {type: \"slider\", min: 0.0, max:1.0, step: 0.05, default: 0.05}\n",
    "\n",
    "\n",
    "# Training and evaluation episodes\n",
    "TRAIN_EPISODES = 1000 #@param {type: \"slider\", min: 1, max: 20000, default: 1000}\n",
    "\n",
    "# Evaluate after specified number of episodes\n",
    "EVAL_EVERY = 10 #@param {type: \"slider\", min: 0, max: 1000}\n",
    "\n",
    "# Evaluate using the specified number of episodes\n",
    "EVAL_EPISODES = 10 #@param {type: \"slider\", min: 1, max: 1000}\n",
    "\n",
    "# Display\n",
    "VERBOSE = False #@param {type: \"boolean\"}\n",
    "PLOT_SCORE = True #@param {type: \"boolean\"}\n",
    "SLEEP_TIME = 1 #@param {type: \"slider\", min:1, max:10}\n",
    "\n",
    "# Show the end result\n",
    "FINAL_SHOW = True #@param {type: \"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HT5G9X_y1FQ6"
   },
   "source": [
    "### Clasă care abstractizează jocul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zquu_dyV1J8J"
   },
   "outputs": [],
   "source": [
    "URL_PREFIX = \"https://raw.githubusercontent.com/cs-pub-ro/ML/master/lab/lab7/maps/\"\n",
    "\n",
    "ACTIONS = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\", \"STAY\"]\n",
    "\n",
    "ACTION_EFFECTS = {\n",
    "    \"UP\": (-1,0),\n",
    "    \"RIGHT\": (0,1),\n",
    "    \"DOWN\": (1,0),\n",
    "    \"LEFT\": (0,-1),\n",
    "    \"STAY\": (0,0)\n",
    "}\n",
    "\n",
    "MOVE_REWARD = -0.1\n",
    "WIN_REWARD = 10.0\n",
    "LOSE_REWARD = -10.0\n",
    "\n",
    "## Functions to serialize / deserialize game states\n",
    "def __serialize_state(state):\n",
    "    return \"\\n\".join(map(lambda row: \"\".join(row), state))\n",
    "\n",
    "def __deserialize_state(str_state):\n",
    "    return list(map(list, str_state.split(\"\\n\")))\n",
    "\n",
    "## Return the initial state of the game\n",
    "def get_initial_state(map_file_name):\n",
    "    full_url = f\"{URL_PREFIX}{MAP_NAME}\"\n",
    "    state = urllib.request.urlopen(full_url).read().strip()\n",
    "\n",
    "    return state.decode(\"utf-8\")\n",
    "\n",
    "## Get the coordinates of an actor\n",
    "def __get_position(state, marker):\n",
    "    for row_idx, row in enumerate(state):\n",
    "        if marker in row:\n",
    "            return row_idx, row.index(marker)\n",
    "    return -1, -1\n",
    "\n",
    "## Check if is a final state\n",
    "def is_final_state(str_state, score):\n",
    "    return score < -20.0 or \"G\" not in str_state or \"o\" not in str_state\n",
    "\n",
    "## Check if the given coordinates are valid (on map and not a wall)\n",
    "def __is_valid_cell(state, row, col):\n",
    "    return row >= 0 and row < len(state) and \\\n",
    "        col >= 0 and col < len(state[row]) and \\\n",
    "        state[row][col] != \"*\"\n",
    "\n",
    "## Move to next state\n",
    "def apply_action(str_state, action):\n",
    "    assert(action in ACTIONS)\n",
    "    message = \"Greuceanu moved %s.\" % action\n",
    "\n",
    "    state = __deserialize_state(str_state)\n",
    "    g_row, g_col = __get_position(state, \"G\")\n",
    "    assert(g_row >= 0 and g_col >= 0)\n",
    "\n",
    "    next_g_row = g_row + ACTION_EFFECTS[action][0]\n",
    "    next_g_col = g_col + ACTION_EFFECTS[action][1]\n",
    "\n",
    "    if not __is_valid_cell(state, next_g_row, next_g_col):\n",
    "        next_g_row = g_row\n",
    "        next_g_col = g_col\n",
    "        message = f\"{message} Not a valid cell there.\"\n",
    "\n",
    "    state[g_row][g_col] = \" \"\n",
    "    if state[next_g_row][next_g_col] == \"B\":\n",
    "        message = f\"{message} Greuceanu stepped on the balaur.\"\n",
    "        return __serialize_state(state), LOSE_REWARD, message\n",
    "    elif state[next_g_row][next_g_col] == \"o\":\n",
    "        state[next_g_row][next_g_col] = \"G\"\n",
    "        message = f\"{message} Greuceanu found 'marul fermecat'.\"\n",
    "        return __serialize_state(state), WIN_REWARD, message\n",
    "    state[next_g_row][next_g_col] = \"G\"\n",
    "\n",
    "    ## Balaur moves now\n",
    "    b_row, b_col = __get_position(state, \"B\")\n",
    "    assert(b_row >= 0 and b_col >= 0)\n",
    "\n",
    "    dy, dx = next_g_row - b_row, next_g_col - b_col\n",
    "\n",
    "    is_good = lambda dr, dc:__is_valid_cell(state, b_row + dr, b_col + dc)\n",
    "\n",
    "    next_b_row, next_b_col = b_row, b_col\n",
    "    if abs(dy) > abs(dx) and is_good(dy // abs(dy), 0):\n",
    "        next_b_row = b_row + dy // abs(dy)\n",
    "    elif abs(dx) > abs(dy) and is_good(0, dx // abs(dx)):\n",
    "        next_b_col = b_col + dx // abs(dx)\n",
    "    else:\n",
    "        options = []\n",
    "        if abs(dx) > 0:\n",
    "            if is_good(0, dx // abs(dx)):\n",
    "                options.append((b_row, b_col + dx // abs(dx)))\n",
    "        else:\n",
    "            if is_good(0, -1):\n",
    "                options.append((b_row, b_col - 1))\n",
    "            if is_good(0, 1):\n",
    "                options.append((b_row, b_col + 1))\n",
    "        if abs(dy) > 0:\n",
    "            if is_good(dy // abs(dy), 0):\n",
    "                options.append((b_row + dy // abs(dy), b_col))\n",
    "        else:\n",
    "            if is_good(-1, 0):\n",
    "                options.append((b_row - 1, b_col))\n",
    "            if is_good(1, 0):\n",
    "                options.append((b_row + 1, b_col))\n",
    "\n",
    "        if len(options) > 0:\n",
    "            next_b_row, next_b_col = choice(options)\n",
    "\n",
    "    if state[next_b_row][next_b_col] == \"G\":\n",
    "        message = f\"{message} The balaur ate Greuceanu.\"\n",
    "        reward = LOSE_REWARD\n",
    "    elif state[next_b_row][next_b_col] == \"o\":\n",
    "        message = f\"{message} The balaur found marul fermecat. Greuceanu lost!\"\n",
    "        reward = LOSE_REWARD\n",
    "    else:\n",
    "        message = f\"{message} The balaur follows Greuceanu.\"\n",
    "        reward = MOVE_REWARD\n",
    "\n",
    "    state[b_row][b_col] = \" \"\n",
    "    state[next_b_row][next_b_col] = \"B\"\n",
    "\n",
    "    return __serialize_state(state), reward, message\n",
    "\n",
    "def display_state(state):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tGs4sOVqfdG"
   },
   "source": [
    "# 3. Problemă de rezolvat\n",
    "## Greuceanu și Balaurul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkAodSkhqrq-"
   },
   "source": [
    "Pe o hartă bidimensională se înfruntă Greuceanu și-un balaur.\n",
    "\n",
    "Greuceanu trebuie să găsească mărul fermecat înainte de a fi prins de balaur și înainte ca balaurul să calce pe măr. Balaurul simte direcția în care se află Greuceanu și se îndreaptă către el.\n",
    "\n",
    "Concret, Greuceanu **câștigă** jocul și 10 puncte dacă ajunge primul la mărul fermecat.\n",
    "\n",
    "Greuceanu **pierde** jocul dacă este prins de balaur sau dacă balaurul calcă pe măr. Deasemenea, la fiecare moment de timp Greuceanu pierde câte 0.1 puncte. \n",
    "\n",
    "Dacă **ajunge la -20 de puncte**, Greuceanu pierde jocul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkW15QQZ1Dco"
   },
   "source": [
    "## 4. Cerințe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6htJ_SIHOKV"
   },
   "source": [
    "1. [6 pct] Implementați algoritmul Q-learning (completași funcția **q_learning**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnfDhv-7Jrd7"
   },
   "source": [
    "2. [2 pct] Implementați strategia $\\epsilon$-greedy de selecție a unei acțiuni. Funcția primește toate acțiunile valide dintr-o stare dată. Atât timp cât există acțiuni ce nu au fost explorate, se va alege aleator una dintre acestea. \n",
    "Altfel, cu o probabilitate $\\epsilon$ se va alege o acțiune aleatoare, iar cu o probabilitate 1 − $\\epsilon$ se va alege cea mai bună acțiune din starea respectivă."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SbAPqv1KSEE"
   },
   "source": [
    "\n",
    "3. [2 pct] Implementați rutina de evaluare a politicii lacome (care alege întotdeauna cea mai bună acțiune)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woN7nPmvJrt1"
   },
   "source": [
    "4. [2 pct] Găsiți metaparametrii potriviți pentru o învățare cât mai rapidă pe toate cele trei hărți (rata de învățare, valoarea lui $\\epsilon$). Încercați să modificați $\\epsilon$ pe parcursul învățarii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBIjvzqHL37B"
   },
   "outputs": [],
   "source": [
    "def get_legal_actions(str_state):\n",
    "    #TODO (1) : Get the actions Greuceanu can do\n",
    "    return deepcopy(ACTIONS)\n",
    "\n",
    "def epsilon_greedy(Q, state, legal_actions, epsilon):\n",
    "    # TODO (2) : Epsilon greedy\n",
    "    return choice(legal_actions)\n",
    "\n",
    "def best_action(Q, state, legal_actions):\n",
    "    # TODO (3) : Best action\n",
    "    return choice(legal_actions)\n",
    "\n",
    "def q_learning():\n",
    "    Q = {}\n",
    "    train_scores = []\n",
    "    eval_scores = []\n",
    "    initial_state = get_initial_state(MAP_NAME)\n",
    "\n",
    "    for train_ep in range(1, TRAIN_EPISODES+1):\n",
    "        clear_output(wait=True)\n",
    "        score = 0\n",
    "        state = deepcopy(initial_state)\n",
    "\n",
    "        if VERBOSE:\n",
    "            display_state(state); sleep(SLEEP_TIME)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        while not is_final_state(state, score):\n",
    "\n",
    "            actions = get_legal_actions(state)\n",
    "            action = epsilon_greedy(Q, state, actions, EPSILON)\n",
    "\n",
    "            state, reward, msg = apply_action(state, action)\n",
    "            score += reward\n",
    "            \n",
    "            # TODO (1) : Q-Learning\n",
    "            if VERBOSE:\n",
    "                print(msg); display_state(state); sleep(SLEEP_TIME)\n",
    "                clear_output(wait=True)\n",
    "\n",
    "\n",
    "        print(f\"Episode {train_ep} / {TRAIN_EPISODES}\")\n",
    "        train_scores.append(score)\n",
    "\n",
    "        # evaluate the greedy policy\n",
    "        if train_ep % EVAL_EVERY == 0:\n",
    "            avg_score = .0\n",
    "\n",
    "            # TODO (4) : Evaluate\n",
    "            eval_scores.append(avg_score)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    if FINAL_SHOW:\n",
    "        state = deepcopy(initial_state)\n",
    "        while not is_final_state(state, score):\n",
    "            action = best_action(Q, state, get_legal_actions(state))\n",
    "            state, _, msg = apply_action(state, action)\n",
    "            print(msg); display_state(state); sleep(SLEEP_TIME)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    if PLOT_SCORE:\n",
    "        from matplotlib import pyplot as plt\n",
    "        import numpy as np\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average score\")\n",
    "        plt.plot(\n",
    "            np.linspace(1, TRAIN_EPISODES, TRAIN_EPISODES),\n",
    "            np.convolve(train_scores, [0.2,0.2,0.2,0.2,0.2], \"same\"),\n",
    "            linewidth = 1.0, color = \"blue\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.linspace(EVAL_EVERY, TRAIN_EPISODES, len(eval_scores)),\n",
    "                        eval_scores, linewidth = 2.0, color = \"red\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "q_learning()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN/uvc0L9CPWm0L1/L+wYs6",
   "collapsed_sections": [
    "HT5G9X_y1FQ6",
    "6tGs4sOVqfdG",
    "KkW15QQZ1Dco"
   ],
   "include_colab_link": true,
   "name": "Laborator 7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
