{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator: Sequence-to-Sequence Learning cu Numere Romane\n",
    "\n",
    "În această sesiune, vom explora **sequence-to-sequence learning**, un concept fundamental în machine learning folosit pentru transformarea unei secvențe în alta. Exemple de astfel de sarcini includ traducerea textului dintr-o limbă în alta, sumarizarea textului sau chiar convertirea numerelor în echivalentul lor în cifre romane. Acest laborator va oferi o introducere practică în sarcinile de sequence-to-sequence folosind **PyTorch**. Vei învăța cum să:\n",
    "\n",
    "1. Reprezinți datele ca secvențe (de exemplu, numere și cifre romane)\n",
    "2. Pregătești un set de date potrivit pentru sequence-to-sequence learning\n",
    "3. Înțelegi cum diferiți parametri ai arhitecturii **GRU** pot afecta învățarea\n",
    "\n",
    "### Numerele Romane ca Studiu de Caz\n",
    "\n",
    "În acest laborator, vom:\n",
    "- Genera un set de date cu numere și reprezentările lor în cifre romane\n",
    "- Examina cum această problemă poate fi încadrată ca o sarcină de sequence-to-sequence\n",
    "- Pregăti datele pentru antrenarea unui model sequence-to-sequence\n",
    "- Antrena și evalua mai multe arhitecturi bazate pe GRU\n",
    "\n",
    "#### Ce Trebuie Să Știi\n",
    "\n",
    "Acest laborator presupune că ai cunoștințe de bază despre:\n",
    "- Lucrul cu biblioteci precum `torch` (PyTorch) pentru machine learning\n",
    "- Concepte despre seturi de date și pregătirea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Înțelegerea Setului de Date: Numere în Numere Romane\n",
    "\n",
    "În această secțiune, pregătim setul de date pentru sarcina noastră de sequence-to-sequence. Scopul este de a crea perechi de intrare (un număr) și ieșire (reprezentarea sa în cifre romane). Aceste perechi vor fi fundamentul pentru antrenarea unui model care poate învăța transformarea.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Convertirea Numerelor în Numere Romane**\n",
    "  - Funcția `to_roman(num)` primește un număr întreg (de exemplu, `58`) și îl convertește în echivalentul său în cifre romane (de exemplu, `LVIII`).\n",
    "  - Folosește o **mapare a simbolurilor numerelor romane** la valorile lor întregi și construiește iterativ numărul roman prin scăderea celei mai mari valori posibile.\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Input: `29`\n",
    "  - Procesul de Conversie:\n",
    "    - Scade `10` → Număr roman: `X`\n",
    "    - Scade `10` → Număr roman: `XX`\n",
    "    - Scade `9` → Număr roman: `XXIX`\n",
    "  - Output: `XXIX`\n",
    "\n",
    "2. **Clasa RomanNumeralDataset**\n",
    "  - Această clasă generează un set de date cu numere aleatorii într-un interval specificat și le convertește în numere romane.\n",
    "  - Utilizează clasa `Dataset` din PyTorch, făcând-o ușor de integrat în pipeline-urile de date pentru machine learning.\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Dacă intervalul este de la 1 la 20, setul de date ar putea arăta astfel:\n",
    "    ```\n",
    "    Input: [1, 4, 12, 19]\n",
    "    Output: ['I', 'IV', 'XII', 'XIX']\n",
    "    ```\n",
    "\n",
    "3. **Atributele Clasei Dataset**\n",
    "  - `numbers`: O listă de numere întregi generate aleator în intervalul dat.\n",
    "  - `roman_numerals`: O listă de șiruri de caractere reprezentând numerele romane corespunzătoare, generate folosind funcția `to_roman`.\n",
    "\n",
    "4. **Accesarea Datelor**\n",
    "  - Folosind `getitem(idx)`, poți accesa un număr specific și echivalentul său în cifre romane ca tuplu:\n",
    "    ```\n",
    "    dataset.getitem(0)  # Exemplu Output: (3, 'III')\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "#### Parcurgerea Codului\n",
    "\n",
    "Iată o detaliere pas cu pas a ceea ce face codul:\n",
    "\n",
    "1. **Inițializarea Setului de Date**:\n",
    "  - Setul de date este inițializat cu un set dat sau aleator de numere (de exemplu, 5.000 de perechi din intervalul 0-10.000).\n",
    "  - Exemplu:\n",
    "    ```python\n",
    "    dataset = RomanNumeralDataset(numbers=random.choices(list(range(10_000)), k=5_000))\n",
    "    ```\n",
    "\n",
    "2. **Generarea Numerelor Aleatoare**:\n",
    "  - Numerele întregi aleatoare pot fi generate folosind `torch.randint` sau `np.random.randint` sau alte metode, asigurând o distribuție uniformă în interval.\n",
    "  - Exemplu Output (aleator): `[25, 178, 3, 944]`\n",
    "\n",
    "3. **Convertirea Numerelor în Numere Romane**:\n",
    "  - Pentru fiecare număr, `to_roman` îl convertește în numărul său roman.\n",
    "  - Exemplu:\n",
    "    ```\n",
    "    Input: [25, 178, 3, 944]\n",
    "    Output: ['XXV', 'CLXXVIII', 'III', 'CMXLIV']\n",
    "    ```\n",
    "\n",
    "4. **Recuperarea Eșantioanelor de Date**:\n",
    "  - Setul de date este iterabil, și fiecare element oferă o pereche `(număr, număr roman)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_roman(num):\n",
    "    roman_values = [\n",
    "        (1000, 'M'),\n",
    "        (900, 'CM'),\n",
    "        (500, 'D'),\n",
    "        (400, 'CD'),\n",
    "        (100, 'C'),\n",
    "        (90, 'XC'),\n",
    "        (50, 'L'),\n",
    "        (40, 'XL'),\n",
    "        (10, 'X'),\n",
    "        (9, 'IX'),\n",
    "        (5, 'V'),\n",
    "        (4, 'IV'),\n",
    "        (1, 'I')\n",
    "    ]\n",
    "    \n",
    "    roman = ''\n",
    "    for value, symbol in roman_values:\n",
    "        while num >= value:\n",
    "            roman += symbol\n",
    "            num -= value\n",
    "    return roman\n",
    "\n",
    "class RomanNumeralDataset(Dataset):\n",
    "    def __init__(self, numbers):\n",
    "        \"\"\"\n",
    "        Create a dataset of numbers and their Roman numeral equivalents.\n",
    "        Args:\n",
    "            numbers (list): List of numbers to include in the dataset.\n",
    "        \"\"\"\n",
    "        self.numbers = numbers\n",
    "        self.roman_numerals = [to_roman(n) for n in self.numbers]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.numbers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.numbers[idx], self.roman_numerals[idx]\n",
    "\n",
    "def create_splits(start=1, end=2025, split_ratio=0.8, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create train and test splits for numbers.\n",
    "    \n",
    "    Args:\n",
    "        start (int): Starting number for the dataset.\n",
    "        end (int): Ending number for the dataset.\n",
    "        split_ratio (float): Ratio of data to use for training (default: 0.8).\n",
    "        shuffle (bool): Whether to shuffle the data before splitting.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training and testing datasets.\n",
    "    \"\"\"\n",
    "    numbers = list(range(start, end + 1))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(numbers)\n",
    "    split_idx = int(len(numbers) * split_ratio)\n",
    "    train_numbers = numbers[:split_idx]\n",
    "    test_numbers = numbers[split_idx:]\n",
    "    \n",
    "    train_dataset = RomanNumeralDataset(train_numbers)\n",
    "    test_dataset = RomanNumeralDataset(test_numbers)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizarea Numerelor Arabe: Clasa `ArabicNumberTokenizer`\n",
    "\n",
    "În sarcinile de sequence-to-sequence, datele de intrare și ieșire trebuie transformate într-un format pe care modelele îl pot procesa. Pentru această sarcină, unde intrarea constă în numere arabe (de exemplu, 1234), avem nevoie de un tokenizer pentru a converti aceste numere în secvențe de token-uri.\n",
    "\n",
    "Clasa `ArabicNumberTokenizer` oferă o modalitate simplă dar eficientă de a tokeniza și detokeniza numere arabe pentru modele de machine learning. Aceasta gestionează:\n",
    "- **Codificarea numerelor în secvențe de token-uri**, adăugând token-uri speciale pentru sfârșitul secvenței (EOS) și padding.\n",
    "- **Decodificarea secvențelor de token-uri înapoi în numere** pentru interpretabilitate.\n",
    "\n",
    "---\n",
    "\n",
    "#### Caracteristici Cheie ale Tokenizer-ului\n",
    "\n",
    "1. **Reprezentarea Token-urilor**\n",
    "  - Fiecare cifră (`0-9`) este mapată la un index unic de token (`0-9`).\n",
    "  - Token-uri speciale:\n",
    "    - `<eos>` (End of Sequence): Index token `10`.\n",
    "    - `<pad>` (Padding): Index token `11`.\n",
    "\n",
    "2. **Codificarea Numerelor**\n",
    "  - Convertește un număr într-o secvență de indici de token-uri.\n",
    "  - Completează numerele cu zerouri în față până la o lungime fixă (`max_digits`) pentru uniformitate.\n",
    "  - Adaugă token-ul `<eos>` la secvență pentru a semnala sfârșitul.\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Input: `123`\n",
    "  - Procesul de Codificare:\n",
    "    - Completare până la `4` cifre: `0123`\n",
    "    - Convertire în token-uri: `[0, 1, 2, 3]`\n",
    "    - Adăugare token EOS: `[0, 1, 2, 3, 10]`\n",
    "  - Output: `torch.tensor([0, 1, 2, 3, 10])`\n",
    "\n",
    "3. **Decodificarea Token-urilor**\n",
    "  - Convertește o secvență de token-uri înapoi în numărul corespunzător.\n",
    "  - Oprește decodificarea la token-ul `<eos>` și ignoră token-urile `<pad>`.\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Input: `[0, 1, 2, 3, 10, 11]`\n",
    "  - Procesul de Decodificare:\n",
    "    - Ignoră padding-ul (`11`).\n",
    "    - Se oprește la EOS (`10`).\n",
    "    - Combină cifrele: `0123`\n",
    "  - Output: `123`\n",
    "\n",
    "4. **Gestionarea Vocabularului**\n",
    "  - Tokenizer-ul definește o **dimensiune a vocabularului** de `12` token-uri:\n",
    "    - `10 cifre` (`0-9`) + `<eos>` + `<pad>`.\n",
    "  - Oferă metode utilitare pentru obținerea indexului token-urilor `<pad>` și `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicNumberTokenizer:\n",
    "    def __init__(self, max_digits=4):  # 4 digits can handle numbers up to 3999\n",
    "        self.max_digits = max_digits\n",
    "        # Add EOS token at index 10, padding at index 11\n",
    "        self.digit_to_idx = {str(i): i for i in range(10)}\n",
    "        self.digit_to_idx['<eos>'] = 10\n",
    "        self.digit_to_idx['<pad>'] = 11\n",
    "        self.idx_to_digit = {i: str(i) for i in range(10)}\n",
    "        self.idx_to_digit[10] = '<eos>'\n",
    "        self.idx_to_digit[11] = '<pad>'\n",
    "\n",
    "    def encode(self, number):\n",
    "        \"\"\"\n",
    "        1. You need to convert the number to a string of fixed length (use zfill)\n",
    "        2. Convert each digit to its corresponding token index using digit_to_idx\n",
    "        3. Don't forget to add the EOS token at the end\n",
    "        4. Return everything as a PyTorch tensor\n",
    "        \n",
    "        Example:\n",
    "        Input number: 42\n",
    "        Padded to 4 digits: \"0042\"\n",
    "        Add EOS: [\"0\", \"0\", \"4\", \"2\", \"<eos>\"]\n",
    "        Convert to indices: [0, 0, 4, 2, 10]\n",
    "        \"\"\"\n",
    "        # Convert number to string\n",
    "        # use zfill to add 0's at the beginning of the string until it reaches `max_digits`\n",
    "        number_str = str(number).zfill(self.max_digits)\n",
    "        \n",
    "        # Convert each digit to the token idx and add EOS token\n",
    "        tokens = [self.digit_to_idx[digit] for digit in number_str] + [self.digit_to_idx['<eos>']]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        1. Convert each token back to its digit representation using idx_to_digit\n",
    "        2. Stop when you hit the EOS token\n",
    "        3. Skip any padding tokens\n",
    "        4. Join the digits together and convert back to integer\n",
    "        \n",
    "        Example:\n",
    "        Input tokens: [0, 0, 4, 2, 10, 11, 11]\n",
    "        Filter special tokens: [\"0\", \"0\", \"4\", \"2\"]\n",
    "        Join and convert: \"0042\" -> 42\n",
    "        \"\"\"\n",
    "        digits = []\n",
    "        for token in tokens:\n",
    "            token = token.item()\n",
    "            \n",
    "            if token == self.digit_to_idx['<eos>']:\n",
    "                break\n",
    "            \n",
    "            if token != self.digit_to_idx['<pad>']:\n",
    "                digits.append(self.idx_to_digit[token])\n",
    "            \n",
    "        return int(''.join(digits))\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return 12  # 0-9 digits plus EOS and padding tokens\n",
    "    \n",
    "    def pad_idx(self):\n",
    "        return 11\n",
    "    \n",
    "    def eos_idx(self):\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_arabic_number_tokenizer():\n",
    "    print(\"Testing ArabicNumberTokenizer...\")\n",
    "    tokenizer = ArabicNumberTokenizer(max_digits=4)\n",
    "\n",
    "    # Test encoding\n",
    "    number = 123\n",
    "    encoded = tokenizer.encode(number)\n",
    "    expected_encoded = torch.tensor([0, 1, 2, 3, 10], dtype=torch.long)  # '0123' + <eos>\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    print(f\"Encode Test Passed: {number} -> {encoded.tolist()}\")\n",
    "    \n",
    "    # Test decoding\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == number, f\"Decoding failed: {decoded} != {number}\"\n",
    "    print(f\"Decode Test Passed: {encoded.tolist()} -> {decoded}\")\n",
    "    \n",
    "    # Edge case: maximum digits\n",
    "    number = 9999\n",
    "    encoded = tokenizer.encode(number)\n",
    "    expected_encoded = torch.tensor([9, 9, 9, 9, 10], dtype=torch.long)  # '9999' + <eos>\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == number, f\"Decoding failed: {decoded} != {number}\"\n",
    "    print(f\"Edge Test Passed: {number} -> {encoded.tolist()} -> {decoded}\")\n",
    "    \n",
    "    # Edge case: padding with zeros\n",
    "    number = 1\n",
    "    encoded = tokenizer.encode(number)\n",
    "    expected_encoded = torch.tensor([0, 0, 0, 1, 10], dtype=torch.long)  # '0001' + <eos>\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == number, f\"Decoding failed: {decoded} != {number}\"\n",
    "    print(f\"Padding Test Passed: {number} -> {encoded.tolist()} -> {decoded}\")\n",
    "\n",
    "test_arabic_number_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizarea Numerelor Romane: Clasa `RomanNumeralTokenizer`\n",
    "\n",
    "Când lucrăm cu sarcini de sequence-to-sequence, datele de ieșire trebuie de asemenea tokenizate într-un format pe care modelele de machine learning îl pot procesa. Pentru sarcina noastră, aceasta înseamnă transformarea numerelor romane (de exemplu, `XXI`) în secvențe de token-uri și invers.\n",
    "\n",
    "Clasa `RomanNumeralTokenizer` este proiectată să gestioneze:\n",
    "- **Codificarea numerelor romane în secvențe de token-uri**, adăugând token-uri speciale pentru sfârșitul secvenței (EOS) și padding.\n",
    "- **Decodificarea secvențelor de token-uri înapoi în numere romane**, permițând interpretabilitatea.\n",
    "\n",
    "---\n",
    "\n",
    "#### Caracteristici Cheie ale Tokenizer-ului\n",
    "\n",
    "1. **Reprezentarea Token-urilor**\n",
    "  - Caracterele numerelor romane (`I`, `V`, `X`, `L`, `C`, `D`, `M`) sunt fiecare mapate la un index unic de token (`0-6`).\n",
    "  - Token-uri speciale:\n",
    "    - `<eos>` (End of Sequence): Index token `7`.\n",
    "    - `<pad>` (Padding): Index token `8`.\n",
    "\n",
    "2. **Codificarea Numerelor Romane**\n",
    "  - Convertește șirurile de numere romane în secvențe de indici de token-uri.\n",
    "  - Adaugă token-ul `<eos>` pentru a semnala sfârșitul secvenței.\n",
    "  - Completează secvența cu token-uri `<pad>` pentru a asigura o lungime uniformă (`max_length`).\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Input: `XII`, `max_length=6`\n",
    "  - Procesul de Codificare:\n",
    "    - Convertire în token-uri: `[2, 0, 0]` (corespunzând lui `X`, `I`, `I`)\n",
    "    - Adăugare token EOS: `[2, 0, 0, 7]`\n",
    "    - Completare până la lungimea `6`: `[2, 0, 0, 7, 8, 8]`\n",
    "  - Output: `torch.tensor([2, 0, 0, 7, 8, 8])`\n",
    "\n",
    "3. **Decodificarea Token-urilor**\n",
    "  - Convertește secvențele de token-uri înapoi în șiruri de numere romane.\n",
    "  - Oprește decodificarea la token-ul `<eos>` și sare peste token-urile `<pad>`.\n",
    "\n",
    "  **Exemplu:**\n",
    "  - Input: `[2, 0, 0, 7, 8, 8]`\n",
    "  - Procesul de Decodificare:\n",
    "    - Ignoră padding-ul (`8`).\n",
    "    - Se oprește la EOS (`7`).\n",
    "    - Combină caracterele: `XII`\n",
    "  - Output: `XII`\n",
    "\n",
    "4. **Gestionarea Vocabularului**\n",
    "  - Tokenizer-ul definește o **dimensiune a vocabularului** de `9` token-uri:\n",
    "    - `7 caractere de numere romane` (`I`, `V`, `X`, `L`, `C`, `D`, `M`) + `<eos>` + `<pad>`.\n",
    "  - Oferă metode utilitare pentru obținerea indexului token-urilor `<pad>` și `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RomanNumeralTokenizer:\n",
    "    def __init__(self):\n",
    "        # Add EOS token at index 7, padding at index 8\n",
    "        self.char_to_idx = {\n",
    "            'I': 0,\n",
    "            'V': 1,\n",
    "            'X': 2,\n",
    "            'L': 3,\n",
    "            'C': 4,\n",
    "            'D': 5,\n",
    "            'M': 6,\n",
    "            '<eos>': 7,\n",
    "            '<pad>': 8\n",
    "        }\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "            \n",
    "\n",
    "    def encode(self, roman_numeral, max_length):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "        Input roman_numeral: \"XIV\"\n",
    "        Convert to tokens + <eos>: [2, 0, 1, 7]\n",
    "        Padd seqeunce with <pad>:  [2, 0, 1, 7, 8]\n",
    "        \"\"\"\n",
    "        # Convert characters to indices and add EOS token\n",
    "        indices = [self.char_to_idx[char] for char in roman_numeral]\n",
    "        indices.append(self.char_to_idx['<eos>'])\n",
    "        \n",
    "        # Pad sequence with padding token index\n",
    "        padded = indices + [self.char_to_idx['<pad>']] * (max_length - len(indices))\n",
    "        return torch.tensor(padded, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        1. Convert each token back to its numeral representation using idx_to_char\n",
    "        2. Stop when you hit the EOS token\n",
    "        3. Skip any padding tokens\n",
    "        4. Join the digits together\n",
    "        \n",
    "        Example:\n",
    "        Input tokens: [2, 0, 1, 7, 8]\n",
    "        Filter special tokens: [2, 0, 1, 7]\n",
    "        Convert and join: XIV\n",
    "        \"\"\"\n",
    "        # Convert indices back to characters until EOS token\n",
    "        chars = []\n",
    "        for token in tokens:\n",
    "            token = token.item()\n",
    "            \n",
    "            if token == self.char_to_idx['<eos>']:\n",
    "                break\n",
    "            \n",
    "            if token != self.char_to_idx['<pad>']:\n",
    "                chars.append(self.idx_to_char[token])\n",
    "            \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.char_to_idx)  # 7 Roman numerals + EOS + padding = 9\n",
    "    \n",
    "    def pad_idx(self):\n",
    "        return 8\n",
    "    \n",
    "    def eos_idx(self):\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_roman_numeral_tokenizer():\n",
    "    print(\"Testing RomanNumeralTokenizer...\")\n",
    "    tokenizer = RomanNumeralTokenizer()\n",
    "    \n",
    "    # Test encoding\n",
    "    roman = \"XIV\"\n",
    "    max_length = 5\n",
    "    encoded = tokenizer.encode(roman, max_length)\n",
    "    expected_encoded = torch.tensor([2, 0, 1, 7, 8], dtype=torch.long)  # 'XIV' + <eos> + <pad>\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    print(f\"Encode Test Passed: {roman} -> {encoded.tolist()}\")\n",
    "    \n",
    "    # Test decoding\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == roman, f\"Decoding failed: {decoded} != {roman}\"\n",
    "    print(f\"Decode Test Passed: {encoded.tolist()} -> {decoded}\")\n",
    "    \n",
    "    # Edge case: short Roman numeral with padding\n",
    "    roman = \"I\"\n",
    "    encoded = tokenizer.encode(roman, max_length)\n",
    "    expected_encoded = torch.tensor([0, 7, 8, 8, 8], dtype=torch.long)  # 'I' + <eos> + <pad> * 3\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == roman, f\"Decoding failed: {decoded} != {roman}\"\n",
    "    print(f\"Padding Test Passed: {roman} -> {encoded.tolist()} -> {decoded}\")\n",
    "    \n",
    "    # Edge case: full sequence with no padding\n",
    "    roman = \"MMXXV\"\n",
    "    max_length = 6\n",
    "    encoded = tokenizer.encode(roman, max_length)\n",
    "    expected_encoded = torch.tensor([6, 6, 2, 2, 1, 7], dtype=torch.long)  # 'MMXXV' + <eos>\n",
    "    assert torch.equal(encoded, expected_encoded), f\"Encoding failed: {encoded} != {expected_encoded}\"\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    assert decoded == roman, f\"Decoding failed: {decoded} != {roman}\"\n",
    "    print(f\"Full Sequence Test Passed: {roman} -> {encoded.tolist()} -> {decoded}\")\n",
    "\n",
    "test_roman_numeral_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence Learning: Modelul `NumberEncoderDecoder`\n",
    "\n",
    "În această secțiune, vom explora în profunzime arhitectura modelului **sequence-to-sequence (seq2seq)** și implementarea sa pentru convertirea numerelor arabe în numere romane. Modelele seq2seq sunt o clasă de rețele neurale proiectate să mapeze o secvență în alta, fiind deosebit de utile pentru sarcini precum traducerea automată, sumarizarea textului și conversia numerelor în reprezentări numerale.\n",
    "\n",
    "---\n",
    "\n",
    "#### Cum Funcționează Modelele Sequence-to-Sequence\n",
    "\n",
    "Modelele seq2seq constau în două componente principale:\n",
    "1. **Encoder**: Comprimă secvența de intrare într-un vector de context de lungime fixă (o stare ascunsă) care reprezintă semnificația intrării.\n",
    "2. **Decoder**: Expandează acest vector de context pentru a genera secvența de ieșire pas cu pas.\n",
    "\n",
    "##### 1. **Encoder**\n",
    "- Encoder-ul primește secvența de intrare $( \\mathbf{x} = (x_1, x_2, \\ldots, x_n) )$, unde fiecare $( x_i )$ este un element al secvenței de intrare (de exemplu, cifre arabe).\n",
    "- Fiecare element de intrare trece printr-un **strat de embedding**, care mapează token-uri discrete în vectori denși, continui:\n",
    " $[\n",
    " \\mathbf{e}_i = \\text{Embedding}(x_i)\n",
    " ]$\n",
    "- Aceste embedding-uri sunt procesate de o rețea neurală recurentă (de exemplu, GRU sau LSTM) pentru a genera stări ascunse:\n",
    " $[\n",
    " \\mathbf{h}_t = f_{\\text{encoder}}(\\mathbf{e}_t, \\mathbf{h}_{t-1})\n",
    " ]$\n",
    " unde $( f_{\\text{encoder}} )$ este funcția recurentă, $( \\mathbf{e}_t )$ este embedding-ul la momentul $( t )$, și $( \\mathbf{h}_{t-1} )$ este starea ascunsă anterioară.\n",
    "- Starea ascunsă finală a encoder-ului, $( \\mathbf{h}_n )$, servește drept **vector de context**, sumarizând întreaga secvență de intrare.\n",
    "\n",
    "##### 2. **Decoder**\n",
    "- Decoder-ul generează secvența de ieșire $( \\mathbf{y} = (y_1, y_2, \\ldots, y_m) )$ câte un token pe rând.\n",
    "- La fiecare pas $( t )$, decoder-ul:\n",
    " 1. Transformă în embedding token-ul anterior $( y_{t-1} )$:\n",
    "    $[\n",
    "    \\mathbf{e}_{t} = \\text{Embedding}(y_{t-1})\n",
    "    ]$\n",
    " 2. Combină acest embedding cu vectorul de context $( \\mathbf{h}_{n} )$ de la encoder și starea ascunsă anterioară a decoder-ului:\n",
    "    $[\n",
    "    \\mathbf{h}_t = f_{\\text{decoder}}(\\mathbf{e}_t, \\mathbf{h}_{t-1})\n",
    "    ]$\n",
    " 3. Proiectează starea ascunsă $( \\mathbf{h}_t )$ în spațiul vocabularului de ieșire pentru a prezice următorul token:\n",
    "    $[\n",
    "    \\mathbf{y}_t = \\text{softmax}(\\mathbf{W} \\cdot \\mathbf{h}_t + \\mathbf{b})\n",
    "    ]$\n",
    "    unde $( \\mathbf{W} )$ și $( \\mathbf{b} )$ sunt parametri care pot fi învățați.\n",
    "\n",
    "---\n",
    "\n",
    "#### Teacher Forcing\n",
    "\n",
    "În timpul antrenării, decoder-ul poate fie:\n",
    "- Să folosească **token-ul corect** $( y_{t-1} )$ ca intrare (teacher forcing), sau\n",
    "- Să folosească **propriul token prezis** $( \\hat{y}_{t-1} )$ din pasul anterior ca intrare.\n",
    "\n",
    "Teacher forcing ajută la stabilizarea antrenării ghidând modelul, dar dependența totală de teacher forcing poate duce la probleme în timpul inferenței, unde nu există valori corecte disponibile.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementarea `NumberEncoderDecoder`\n",
    "\n",
    "Modelul `NumberEncoderDecoder` este implementat folosind framework-ul seq2seq:\n",
    "\n",
    "1. **Encoder**:\n",
    "  - Transformă în embedding token-urile numerelor arabe folosind `nn.Embedding`.\n",
    "  - Procesează secvența de embedding-uri folosind `nn.GRU` pentru a produce vectorul de context.\n",
    "\n",
    "2. **Decoder**:\n",
    "  - Transformă în embedding token-urile numerelor romane țintă.\n",
    "  - Le procesează pas cu pas folosind un alt `nn.GRU`.\n",
    "  - Proiectează ieșirile GRU în dimensiunea vocabularului de numere romane folosind `nn.Linear`.\n",
    "\n",
    "3. **Teacher Forcing**:\n",
    "  - Un mecanism probabilistic (controlat de `teacher_forcing_ratio`) determină dacă se folosesc token-urile corecte sau predicțiile modelului ca intrări în timpul decodificării.\n",
    "\n",
    "---\n",
    "\n",
    "#### Formularea Matematică a Modelului\n",
    "\n",
    "Fie $( \\mathbf{x} = (x_1, x_2, \\ldots, x_n) )$ token-urile arabe de intrare și $( \\mathbf{y} = (y_1, y_2, \\ldots, y_m) )$ token-urile țintă ale numerelor romane.\n",
    "\n",
    "1. **Encoder**:\n",
    "  $[\n",
    "  \\mathbf{h}_t^{\\text{enc}} = \\text{GRU}_{\\text{enc}}(\\text{Embedding}(x_t), \\mathbf{h}_{t-1}^{\\text{enc}})\n",
    "  ]$\n",
    "  Starea ascunsă finală $( \\mathbf{h}_n^{\\text{enc}} )$ este transmisă decoder-ului.\n",
    "\n",
    "2. **Decoder (la fiecare pas)**:\n",
    "  - Inițializează prima stare ascunsă cu starea finală a encoder-ului:\n",
    "    $[\n",
    "    \\mathbf{h}_0^{\\text{dec}} = \\mathbf{h}_n^{\\text{enc}}\n",
    "    ]$\n",
    "  - Transformă în embedding intrarea decoder-ului:\n",
    "    $[\n",
    "    \\mathbf{e}_t^{\\text{dec}} = \\text{Embedding}(y_{t-1})\n",
    "    ]$\n",
    "  - Actualizează starea ascunsă a decoder-ului:\n",
    "    $[\n",
    "    \\mathbf{h}_t^{\\text{dec}} = \\text{GRU}_{\\text{dec}}(\\mathbf{e}_t^{\\text{dec}}, \\mathbf{h}_{t-1}^{\\text{dec}})\n",
    "    ]$\n",
    "  - Prezice următorul token:\n",
    "    $[\n",
    "    \\hat{\\mathbf{y}}_t = \\text{softmax}(\\mathbf{W} \\cdot \\mathbf{h}_t^{\\text{dec}} + \\mathbf{b})\n",
    "    ]$\n",
    "\n",
    "#### Inițializarea Stării Ascunse $( \\mathbf{h}_0 )$\n",
    "\n",
    "V-ați putea întreba cum este inițializată starea ascunsă $( \\mathbf{h}_0 )$ a encoder-ului GRU, având în vedere că implementarea nu definește explicit un parametru care poate fi învățat pentru acest scop. GRU-ul PyTorch gestionează inițializarea stării ascunse automat prin inițializarea $( \\mathbf{h}_0 )$ cu zerouri. Deși acest lucru ar putea părea prea simplist, este adesea eficient din mai multe motive:\n",
    "\n",
    "1. **Parametri care Pot Fi Învățați în Porțile GRU**:\n",
    "  - GRU conține ponderi care pot fi învățate în cadrul porților și transformărilor sale. Acestea permit GRU-ului să transforme inițializarea cu zerouri în reprezentări semnificative chiar de la primul pas temporal.\n",
    "\n",
    "2. **Embedding-uri Bogate de Intrare**:\n",
    "  - Stratul de embedding (`arabic_embedding`) oferă reprezentări vectoriale dense, învățate ale token-urilor de intrare. Aceste embedding-uri transportă informații semnificative, permițând GRU-ului să derive rapid stări ascunse semnificative.\n",
    "\n",
    "3. **Mecanismul Porții de Actualizare**:\n",
    "  - GRU-urile includ o poartă de actualizare care poate ajusta eficient starea ascunsă bazată pe intrarea și contextul curent. Acest mecanism permite modelului să depășească potențialele limitări ale inițializării cu zerouri.\n",
    "\n",
    "În practică, această abordare funcționează bine în multe scenarii, inclusiv în arhitectura `NumberEncoderDecoder`. Cu toate acestea, dacă este necesar, strategii personalizate de inițializare (de exemplu, stări inițiale care pot fi învățate) pot fi adăugate pentru sarcini sau seturi de date specifice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completarea Implementării Modelului `NumberEncoderDecoder`\n",
    "\n",
    "În această parte, veți completa secțiunile marcate cu `TODO` din modelul `NumberEncoderDecoder`. Aceste `TODO`-uri sunt concepute pentru a vă ghida prin implementarea unui model sequence-to-sequence pentru conversia numerelor arabe în numere romane.\n",
    "\n",
    "---\n",
    "\n",
    "#### De Rezolvat\n",
    "\n",
    "1. **Inițializarea Intrării Decoder-ului**:\n",
    "  - În metoda `forward`, înlocuiți `TODO` pentru a inițializa intrarea decoder-ului cu primul token din secvența țintă (`tgt`).\n",
    "\n",
    "2. **Transformarea în Embedding a Intrării Decoder-ului**:\n",
    "  - În bucla de decodificare din metoda `forward`, înlocuiți `TODO` pentru a transforma în embedding intrarea decoder-ului folosind stratul de embedding pentru numerele romane (`self.roman_embedding`).\n",
    "\n",
    "3. **Decodificarea Un Pas**:\n",
    "  - Înlocuiți `TODO` pentru a efectua un pas de decodificare folosind decoder-ul GRU (`self.decoder`).\n",
    "\n",
    "4. **Proiecția la Dimensiunea Vocabularului**:\n",
    "  - Înlocuiți `TODO` pentru a mapa ieșirea GRU la dimensiunea vocabularului de numere romane folosind stratul de proiecție de ieșire (`self.output_layer`).\n",
    "\n",
    "5. **Stocarea Predicțiilor**:\n",
    "  - Înlocuiți `TODO` pentru a salva predicțiile în tensorul `decoder_outputs`.\n",
    "\n",
    "6. **Implementarea Teacher Forcing**:\n",
    "  - Înlocuiți `TODO` în secțiunea de teacher forcing pentru a decide următoarea `decoder_input`:\n",
    "    - Cu o probabilitate controlată de `teacher_forcing_ratio`, folosiți următorul token din `tgt`.\n",
    "    - În caz contrar, folosiți token-ul prezis de model.\n",
    "\n",
    "7. **Generarea Token-urilor Fără Teacher Forcing**:\n",
    "  - În metoda `generate`:\n",
    "    - Înlocuiți `TODO` pentru a efectua decodificarea folosind `decode_step`.\n",
    "    - Înlocuiți `TODO` pentru a actualiza `decoder_input` cu token-ul prezis.\n",
    "    - Înlocuiți `TODO` pentru a adăuga token-ul prezis în lista `predictions`.\n",
    "    - Înlocuiți `TODO` pentru a concatena predicțiile într-un tensor la sfârșitul buclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NumberEncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arabic_vocab_size,\n",
    "        roman_vocab_size,\n",
    "        embedding_dim=32,\n",
    "        hidden_size=128,\n",
    "        num_layers=1,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get padding indices\n",
    "        self.arabic_pad_idx = arabic_vocab_size - 1\n",
    "        self.roman_pad_idx = roman_vocab_size - 1\n",
    "        \n",
    "        # Encoder components\n",
    "        self.arabic_embedding = nn.Embedding(\n",
    "            num_embeddings=arabic_vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=self.arabic_pad_idx\n",
    "        )\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        # Decoder components\n",
    "        self.roman_embedding = nn.Embedding(\n",
    "            num_embeddings=roman_vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=self.roman_pad_idx\n",
    "        )\n",
    "        self.decoder = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(hidden_size, roman_vocab_size)\n",
    "        \n",
    "        # Save dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: Tensor of Arabic number tokens [batch_size, src_len]\n",
    "        tgt: Tensor of Roman numeral tokens [batch_size, tgt_len]\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "\n",
    "        \"\"\"\n",
    "        HINTS for each TODO:\n",
    "\n",
    "        1. Initialize decoder_input:\n",
    "        - You need the first token of each sequence in the batch\n",
    "        - Shape should be [batch_size, 1]\n",
    "\n",
    "        2. Embed decoder_input:\n",
    "        - Use self.roman_embedding\n",
    "        - Make sure input shape matches what embedding layer expects\n",
    "        - Output should be [batch_size, 1, embedding_dim]\n",
    "\n",
    "        3. Decode step:\n",
    "        - Use self.decoder\n",
    "        - Need both embedded input and previous hidden state\n",
    "        - Remember that GRU returns (output, hidden_state)\n",
    "\n",
    "        4. Project to vocabulary size:\n",
    "        - Use self.output_layer\n",
    "        - May need to adjust dimensions before projection\n",
    "        - Output should have scores for each possible token\n",
    "\n",
    "        5. Store prediction:\n",
    "        - Save at the correct time step in decoder_outputs\n",
    "        - Watch dimensions, especially the time dimension\n",
    "\n",
    "        6. Teacher forcing:\n",
    "        - When using ground truth: take next token from tgt sequence\n",
    "        - Shape should match decoder_input's expected shape [batch_size, 1]\n",
    "\n",
    "        7. Without teacher forcing:\n",
    "        - Use the model's prediction\n",
    "        - Convert logits to token indices using argmax\n",
    "        - Reshape to match decoder_input's expected shape\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode\n",
    "        src_embedded = self.arabic_embedding(src)\n",
    "        encoder_outputs, hidden = self.encoder(src_embedded)\n",
    "        \n",
    "        # TODO 1.1: Initialize decoder input with start token (can be the first target token)\n",
    "        # decoder_input = ...  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Prepare tensor for all decoder outputs\n",
    "        decoder_outputs = torch.zeros(\n",
    "            batch_size,\n",
    "            tgt_len,\n",
    "            self.output_layer.out_features,\n",
    "            device=src.device\n",
    "        )\n",
    "        \n",
    "        # Decode one step at a time\n",
    "        for t in range(tgt_len):\n",
    "            # TODO 1.2: Embed decoder input\n",
    "            # decoder_embedded = ...\n",
    "            \n",
    "            # TODO 1.3: Decode one step\n",
    "            # output, hidden = ...\n",
    "            \n",
    "            # TODO 1.4: Project to vocabulary size\n",
    "            # prediction = ...\n",
    "            \n",
    "            # TODO 1.5: Save prediction\n",
    "            # decoder_outputs[:, t:t+1] = ...\n",
    "            \n",
    "            # Teacher forcing: use real target tokens as next input\n",
    "            # \n",
    "            # with probability teacher_forcing_ratio\n",
    "            if t < tgt_len - 1:\n",
    "                if torch.rand(1).item() < teacher_forcing_ratio or not self.training:\n",
    "                    # TODO 1.6: Use next token from tgt sequence\n",
    "                    # decoder_input = ...\n",
    "                    pass\n",
    "                else:\n",
    "                    # TODO 1.7: Use our prediction\n",
    "                    # decoder_input = ...\n",
    "                    pass\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"Encode source sequence to hidden state\"\"\"\n",
    "        src_embedded = self.arabic_embedding(src)\n",
    "        _, hidden = self.encoder(src_embedded)\n",
    "        return hidden\n",
    "    \n",
    "    def decode_step(self, decoder_input, hidden):\n",
    "        \"\"\"Perform one decoder step\"\"\"\n",
    "        decoder_embedded = self.roman_embedding(decoder_input)\n",
    "        output, hidden = self.decoder(decoder_embedded, hidden)\n",
    "        prediction = self.output_layer(output)\n",
    "        return prediction, hidden\n",
    "    \n",
    "    def generate(self, src, max_length=20):\n",
    "        \"\"\"Generate sequence without teacher forcing\"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        HINTS for each TODO:\n",
    "\n",
    "        1. Get prediction and hidden state:\n",
    "        - Use self.decode_step\n",
    "        - Pass in current decoder_input and hidden state\n",
    "        - Returns prediction logits and new hidden state\n",
    "\n",
    "        2. Update decoder_input:\n",
    "        - Convert prediction logits to token indices\n",
    "        - Shape should be [batch_size, 1]\n",
    "\n",
    "        3. Save prediction:\n",
    "        - Add current prediction to predictions list\n",
    "        - No need to modify shape here\n",
    "\n",
    "        4. Concatenate predictions:\n",
    "        - Use torch.cat on the predictions list\n",
    "        - Concatenate along the sequence dimension (dim=1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode\n",
    "        hidden = self.encode(src)\n",
    "        \n",
    "        # Initialize decoder input\n",
    "        decoder_input = torch.zeros(\n",
    "            (batch_size, 1),\n",
    "            dtype=torch.long,\n",
    "            device=src.device\n",
    "        )\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions = []\n",
    "        \n",
    "        # Generate tokens until max_length or EOS token\n",
    "        for _ in range(max_length):\n",
    "            # TODO 2.1: Get prediction\n",
    "            # prediction, hidden = ...\n",
    "                        \n",
    "            # TODO 2.2: Get most likely token\n",
    "            # decoder_input = ...\n",
    "            \n",
    "            # TODO 2.3: Save prediction\n",
    "            # ...\n",
    "            \n",
    "            # Stop if we predict EOS token\n",
    "            if (decoder_input == self.roman_pad_idx - 1).all():  # EOS is one before PAD\n",
    "                break\n",
    "        \n",
    "        # TODO 2.4: Concatenate all predictions. Hint: torch.cat\n",
    "        return torch.tensor([])  # Default value, to be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_number_encoder_decoder():\n",
    "    # Define test parameters\n",
    "    batch_size = 2\n",
    "    src_len = 4\n",
    "    tgt_len = 6\n",
    "    arabic_vocab_size = 12  # Example: digits (0-9) + EOS + PAD\n",
    "    roman_vocab_size = 10   # Example: I, V, X, L, C, D, M + EOS + PAD\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = NumberEncoderDecoder(arabic_vocab_size, roman_vocab_size)\n",
    "    \n",
    "    # Generate dummy input and target sequences\n",
    "    src = torch.randint(0, arabic_vocab_size - 2, (batch_size, src_len))  # Arabic tokens\n",
    "    tgt = torch.randint(0, roman_vocab_size - 2, (batch_size, tgt_len))   # Roman tokens\n",
    "\n",
    "    # Perform a forward pass\n",
    "    outputs = model(src, tgt, teacher_forcing_ratio=0.5)\n",
    "\n",
    "    # Check the output shape\n",
    "    expected_shape = (batch_size, tgt_len, roman_vocab_size)\n",
    "    assert outputs.shape == expected_shape, f\"Output shape mismatch. Expected {expected_shape}, got {outputs.shape}\"\n",
    "\n",
    "    # Test the encode method\n",
    "    hidden = model.encode(src)\n",
    "    assert hidden.shape == (model.num_layers, batch_size, model.hidden_size), (\n",
    "        f\"Encode hidden shape mismatch. Expected ({model.num_layers}, {batch_size}, {model.hidden_size}), got {hidden.shape}\"\n",
    "    )\n",
    "\n",
    "    # Test the decode_step method\n",
    "    decoder_input = tgt[:, 0:1]  # First token of the target\n",
    "    prediction, _ = model.decode_step(decoder_input, hidden)\n",
    "    assert prediction.shape == (batch_size, 1, roman_vocab_size), (\n",
    "        f\"Decode step output shape mismatch. Expected ({batch_size}, 1, {roman_vocab_size}), got {prediction.shape}\"\n",
    "    )\n",
    "\n",
    "    # Test the generate method\n",
    "    generated = model.generate(src, max_length=8)\n",
    "    assert generated.shape[0] == batch_size, f\"Generated batch size mismatch. Expected {batch_size}, got {generated.shape[0]}\"\n",
    "    assert generated.shape[1] <= 8, f\"Generated sequence length exceeds max_length. Expected <= 8, got {generated.shape[1]}\"\n",
    "\n",
    "    print(\"All tests passed successfully!\")\n",
    "\n",
    "# Run the test\n",
    "test_number_encoder_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating Roman Numeral Conversion Models\n",
    "\n",
    "In this exercise, you will train and evaluate sequence-to-sequence models for converting Arabic numbers into Roman numerals. The provided code includes:\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "   - `create_splits`: Splits the dataset into training and evaluation sets based on a specified ratio. The split can be shuffled for better generalization.\n",
    "\n",
    "2. **Custom Collate Function**:\n",
    "   - `collate_fn`: Prepares batches of Arabic numbers and Roman numerals by tokenizing and padding the sequences.\n",
    "\n",
    "3. **Training Functionality**:\n",
    "   - `train_epoch`: Performs a single epoch of training and computes the average training loss. You will encounter **TODOs** related to masking non-padding tokens to ensure the loss calculation is accurate.\n",
    "   - `train_model`: Trains the model over multiple epochs using the provided configuration. The function handles evaluation after each epoch and saves the best-performing model.\n",
    "\n",
    "4. **Evaluation Functionality**:\n",
    "   - `evaluate`: Computes both the token-level loss and **full sample accuracy**, ensuring that the entire Roman numeral must be correct for the prediction to count as accurate. You will encounter **TODOs** where specific components need to be completed, such as decoding and loss calculation.\n",
    "\n",
    "5. **Comparison of Configurations**:\n",
    "   - Two configurations (`config` and `config_small`) are provided, allowing you to train models with different parameter settings. These configurations include the embedding size, hidden size, number of layers, dropout, and learning rate.\n",
    "\n",
    "6. **Metric Visualization**:\n",
    "   - `plot_metrics`: Generates side-by-side visualizations of training and evaluation losses and accuracies for multiple configurations. The lines are color-coded and styled (solid for training metrics, dashed for evaluation metrics) to make comparisons clearer.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Tasks\n",
    "\n",
    "1. **Complete the TODOs in `train_epoch`**:\n",
    "   - Implement masking to exclude padding tokens from loss calculations.\n",
    "   - Ensure that the output and target tensors are properly filtered using the `mask`.\n",
    "\n",
    "2. **Complete the TODOs in `evaluate`**:\n",
    "   - Decode predictions and targets using the Roman numeral tokenizer.\n",
    "   - Ensure accurate comparison for full sample accuracy.\n",
    "   - Implement loss calculation by masking padding tokens, similar to `train_epoch`.\n",
    "\n",
    "3. **Compare Configurations**:\n",
    "   - Analyze the difference in training and evaluation metrics between the `config` (base model) and `config_small` (smaller model).\n",
    "   - Identify which configuration performs better and why. Consider parameters like model size, embedding dimension, and hidden layers.\n",
    "\n",
    "4. **Analyze Generalization**:\n",
    "   - Observe how well the models generalize to the evaluation set created using `create_splits`.\n",
    "   - Reflect on the impact of the 95% split ratio on generalization and how shuffling affects the train-test split.\n",
    "\n",
    "5. **Experiment with New Configurations**:\n",
    "   - Modify the configurations (e.g., learning rate, embedding size, or number of layers) and retrain the models.\n",
    "   - Use `plot_metrics` to compare the results of your custom configurations with the provided ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "- Understand how masking is used to exclude padding tokens from loss calculations.\n",
    "- Learn how to compute full sample accuracy in sequence-to-sequence tasks.\n",
    "- Gain hands-on experience with training and evaluating models using different configurations.\n",
    "- Develop intuition about the relationship between model complexity and performance on training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to process batches for DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): A batch of (number, roman numeral) pairs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple of tokenized Arabic numbers and Roman numerals.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizers\n",
    "    arabic_tokenizer = ArabicNumberTokenizer()\n",
    "    roman_tokenizer = RomanNumeralTokenizer()\n",
    "    \n",
    "    # Unzip the batch\n",
    "    numbers, romans = zip(*batch)\n",
    "    \n",
    "    # Get max length for padding (add 1 for EOS token)\n",
    "    max_length = max(len(r) for r in romans) + 1\n",
    "    \n",
    "    # Tokenize all items in batch\n",
    "    number_tokens = torch.stack([arabic_tokenizer.encode(n) for n in numbers])\n",
    "    roman_tokens = torch.stack([roman_tokenizer.encode(r, max_length) for r in romans])\n",
    "    \n",
    "    return number_tokens, roman_tokens\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Perform one epoch of training.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        dataloader (DataLoader): Dataloader for the training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        device (torch.device): Device for training (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    HINTS for each TODO:\n",
    "    \n",
    "    1. Create the mask:\n",
    "    - Need to identify which positions are not padding tokens\n",
    "    \n",
    "    2. Apply the mask:\n",
    "    - Use the mask to select only non-padding positions from output and target\n",
    "    - Make sure output and target shapes match after masking\n",
    "    \"\"\"\n",
    "\n",
    "    roman_tokenizer = RomanNumeralTokenizer()\n",
    "    pad_idx = roman_tokenizer.pad_idx()\n",
    "    \n",
    "    with tqdm(dataloader, desc=\"Training\", leave=False) as pbar:\n",
    "        for src, tgt in pbar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Reshape output and target for loss calculation\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = tgt.view(-1)\n",
    "            \n",
    "            # TODO 3: Calculate loss only on non-padding tokens\n",
    "            # TODO 3.1: Create mask for selecting only non-padding tokens\n",
    "            # mask = ...\n",
    "            \n",
    "            # TODO 3.2: Select output based on mask\n",
    "            # output = \n",
    "            \n",
    "            # TODO 3.3: Select target based on mask\n",
    "            # target = ...\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model with full sample accuracy and token-level loss displayed in tqdm.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to evaluate.\n",
    "        dataloader (DataLoader): Dataloader for evaluation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device for evaluation (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Average evaluation loss and full sample accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    roman_tokenizer = RomanNumeralTokenizer()\n",
    "    pad_idx = roman_tokenizer.pad_idx()\n",
    "    eos_idx = roman_tokenizer.eos_idx()\n",
    "\n",
    "    \"\"\"\n",
    "    1. Decoding predictions and targets:\n",
    "    - Use roman_tokenizer.decode() for both\n",
    "    - Remember that decode() handles EOS and padding tokens automatically\n",
    "    - Correct accuracy, where accuracy is defined by a full match between the expected roman numeral and the predicted one\n",
    "    \n",
    "    2. Model predictions with teacher forcing:\n",
    "    - Use model's forward method with src and tgt\n",
    "    - Remember to properly mask out in order to ignore padding tokens\n",
    "    - Remember to reshape for loss calculation\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc=\"Evaluating\", leave=False) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in pbar:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                \n",
    "                # Generate sequences without teacher forcing\n",
    "                predictions = model.generate(src)\n",
    "                \n",
    "                # Compare full sequences for accuracy\n",
    "                for i, (pred, target) in enumerate(zip(predictions, tgt)):\n",
    "                    \n",
    "                    # Decode both predictions and targets for comparison\n",
    "                    pred_decoded = roman_tokenizer.decode(pred)\n",
    "                    target_decoded = roman_tokenizer.decode(target)\n",
    "                    \n",
    "                    # Increment accuracy counters\n",
    "                    if pred_decoded == target_decoded:\n",
    "                        correct_samples += 1\n",
    "                    total_samples += 1\n",
    "\n",
    "                    # If it's the last sample in the batch, print details\n",
    "                    if i == len(predictions) - 1:\n",
    "                        print(f\"Generated: {pred_decoded}, Expected: {target_decoded}\")\n",
    "                \n",
    "                # For loss calculation, use teacher forcing\n",
    "                \n",
    "                # Get model output for current (src, tgt)\n",
    "                output = model(src, tgt)\n",
    "                \n",
    "                # Flatten output and target \n",
    "                # [batch_size, tgt_len, vocab_size] -> [batch_size * tgt_len, vocab_size]\n",
    "                output_flat = output.view(-1, output.shape[-1])\n",
    "                \n",
    "                # [batch_size, tgt_len] -> [batch_size * tgt_len]\n",
    "                target_flat = tgt.view(-1)\n",
    "                \n",
    "\n",
    "                # Create mask for selecting only non-padding tokens\n",
    "                mask = (target_flat != pad_idx)\n",
    "\n",
    "                # Select output based on mask\n",
    "                output_flat = output_flat[mask]\n",
    "\n",
    "                # Select target based on mask\n",
    "                target_flat = target_flat[mask]\n",
    "                \n",
    "                loss = criterion(output_flat, target_flat)\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    full_sample_accuracy = correct_samples / total_samples if total_samples > 0 else 0\n",
    "    return avg_loss, full_sample_accuracy \n",
    "\n",
    "\n",
    "def plot_metrics(metrics_list, metric_names, zoom_epochs=10):\n",
    "    \"\"\"\n",
    "    Plot multiple metrics from different runs in two separate figures:\n",
    "    1. Full metrics plot (train/eval loss and eval accuracy).\n",
    "    2. A separate plot focusing on the last `zoom_epochs` of training and evaluation losses.\n",
    "    \n",
    "    Args:\n",
    "        metrics_list (list of dict): List of metrics dictionaries, each containing \"train_losses\",\n",
    "                                     \"eval_losses\", and \"eval_accuracies\".\n",
    "        metric_names (list of str): Names for each run to include in the legend.\n",
    "        zoom_epochs (int): Number of final epochs to zoom in on for the second figure.\n",
    "    \"\"\"\n",
    "    colors = cm.get_cmap('Dark2', len(metrics_list))\n",
    "\n",
    "    # Determine maximum epochs and zoomed range\n",
    "    max_epochs = len(metrics_list[0][\"train_losses\"])\n",
    "    start_epoch = max(0, max_epochs - zoom_epochs)\n",
    "    epoch_range = range(start_epoch, max_epochs)\n",
    "\n",
    "    # --- Figure 1: Full metrics ---\n",
    "    fig1, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot losses (full range)\n",
    "    for i, (metrics, name) in enumerate(zip(metrics_list, metric_names)):\n",
    "        color = colors(i)\n",
    "        ax_loss.plot(metrics[\"train_losses\"], label=f'{name} Train Loss', linestyle='-', color=color)\n",
    "        ax_loss.plot(metrics[\"eval_losses\"], label=f'{name} Eval Loss', linestyle='--', color=color)\n",
    "    ax_loss.set_xlabel('Epoch', fontsize=12)\n",
    "    ax_loss.set_ylabel('Loss', fontsize=12)\n",
    "    ax_loss.set_title('Training and Evaluation Losses', fontsize=14)\n",
    "    ax_loss.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    ax_loss.legend(fontsize=10)\n",
    "\n",
    "    # Plot accuracies (full range)\n",
    "    for i, (metrics, name) in enumerate(zip(metrics_list, metric_names)):\n",
    "        color = colors(i)\n",
    "        ax_acc.plot(metrics[\"eval_accuracies\"], label=f'{name} Eval Accuracy', linestyle='-', color=color)\n",
    "    ax_acc.set_xlabel('Epoch', fontsize=12)\n",
    "    ax_acc.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax_acc.set_title('Evaluation Accuracy', fontsize=14)\n",
    "    ax_acc.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    ax_acc.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Figure 2: Zoomed in on the last zoom_epochs ---\n",
    "    fig2, ax_zoom = plt.subplots(figsize=(7, 6))\n",
    "    for i, (metrics, name) in enumerate(zip(metrics_list, metric_names)):\n",
    "        color = colors(i)\n",
    "        ax_zoom.plot(epoch_range, [metrics[\"train_losses\"][e] for e in epoch_range], \n",
    "                     label=f'{name} Train Loss', linestyle='-', color=color)\n",
    "        ax_zoom.plot(epoch_range, [metrics[\"eval_losses\"][e] for e in epoch_range],\n",
    "                     label=f'{name} Eval Loss', linestyle='--', color=color)\n",
    "    ax_zoom.set_xlabel('Epoch', fontsize=12)\n",
    "    ax_zoom.set_ylabel('Loss', fontsize=12)\n",
    "    ax_zoom.set_title(f'Training and Evaluation Losses (Last {zoom_epochs} Epochs)', fontsize=14)\n",
    "    ax_zoom.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    ax_zoom.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(config, train_dataloader, eval_dataloader, device):\n",
    "    \"\"\"\n",
    "    Train the model for a given number of epochs and return the trained model and metrics.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary with the following keys:\n",
    "            - num_epochs (int): Number of epochs to train.\n",
    "            - learning_rate (float): Learning rate for the optimizer.\n",
    "            - embedding_dim (int): Dimension of embeddings.\n",
    "            - hidden_size (int): Hidden size of GRU layers.\n",
    "            - num_layers (int): Number of GRU layers.\n",
    "            - dropout (float): Dropout rate for GRU layers.\n",
    "        train_dataloader (DataLoader): Dataloader for training data.\n",
    "        eval_dataloader (DataLoader): Dataloader for evaluation data.\n",
    "        device (torch.device): Device for training (CPU or GPU).\n",
    "        \n",
    "    Returns:\n",
    "        model (nn.Module): Trained model.\n",
    "        metrics (dict): A dictionary containing training and evaluation losses and accuracies.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizers\n",
    "    arabic_tokenizer = ArabicNumberTokenizer()\n",
    "    roman_tokenizer = RomanNumeralTokenizer()\n",
    "    \n",
    "    # Create model\n",
    "    model = NumberEncoderDecoder(\n",
    "        arabic_vocab_size=arabic_tokenizer.vocab_size(),\n",
    "        roman_vocab_size=roman_tokenizer.vocab_size(),\n",
    "        embedding_dim=config[\"embedding_dim\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=roman_tokenizer.pad_idx())\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    eval_accuracies = []\n",
    "    best_eval_accuracy = 0\n",
    "    \n",
    "    for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "        print(f'Epoch {epoch}/{config[\"num_epochs\"]}')\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_loss, eval_accuracy = evaluate(model, eval_dataloader, criterion, device)\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_accuracies.append(eval_accuracy)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if eval_accuracy > best_eval_accuracy:\n",
    "            best_eval_accuracy = eval_accuracy\n",
    "            torch.save(model.state_dict(), f'model_best.pt')\n",
    "            print('Model checkpoint saved!')\n",
    "    \n",
    "    # Return the trained model and metrics\n",
    "    metrics = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "        \"eval_accuracies\": eval_accuracies\n",
    "    }\n",
    "    return model, metrics\n",
    "\n",
    "# Define the configuration dictionary\n",
    "small_model_config = {\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"embedding_dim\": 16,\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "large_model_config = {\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"embedding_dim\": 32,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 4,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_dataset, eval_dataset = create_splits(start=1, end=2025, split_ratio=0.95, shuffle=True)\n",
    "\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,  shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Train the model\n",
    "trained_model, metrics = train_model(config=small_model_config, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, device=device)\n",
    "trained_model_s, metrics_s = train_model(config=large_model_config, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, device=device)\n",
    "\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics([metrics, metrics_s], ['small', 'large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
