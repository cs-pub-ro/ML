{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfIdnkh92358"
   },
   "source": [
    "# Lab 8 - RNN\n",
    "\n",
    "This is a tutorial training various RNNs on simple datasets and doing some analysis.\n",
    "In acest laborator vom parcurge notiuni introductive privind retelele recurente si folosirea lor pentru a prezice valori viitoare ale unei serii (i.e. setup predictiv - de regresie).\n",
    "\n",
    "Structura:\n",
    "  1. Implementarea unei celule RNN simple\n",
    "  2. Implementarea unui model folosind o celula LSTM\n",
    "  3. Intelegerea conceptului de **teacher forcing** si **warm start** (in cadrul predictiei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA_K3_OL3EY4"
   },
   "source": [
    "## Importuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "j5YGV2hb2RIt"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import LSTM\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "sns.set_style('ticks')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0KPZdiq5AVJ"
   },
   "source": [
    "# Task 1.    Vanilla RNN\n",
    "\n",
    "Implementati o celula RNN de baza folosind straturi liniari din PyTorch.\n",
    "\n",
    "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "   \n",
    "   Unde\n",
    "   \n",
    "   * $x_t$ inputul la momentul $t$ de dimensiunea (batch_size, input_features)\n",
    "   * $h_t$ hidden state at time $t$ de dimensiunea (batch_size, hidden_size)\n",
    "   * $W$ proiectie input-to-hidden (antrenabil) \n",
    "   * $V$ proiectie hidden-to-hidden (antrenabil)\n",
    "   * $b$ bias (antrenabil)\n",
    "   * $f$ functia non-lineara aleasa (de regula tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "z6GIHgOnzd8Y"
   },
   "outputs": [],
   "source": [
    "#@title Vanilla RNN\n",
    "class VanillaRNNCell(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, activation=nn.Tanh, bias=True):    \n",
    "        \"\"\"\n",
    "        Constructor pentru o celula simpla RNNCell unde tranzitiile hidden-to-hidden \n",
    "        sunt definite printr-un strat liniar, iar activarea implicita este `tanh` \n",
    "        :param hidden_size: dimensiunea starii ascunse\n",
    "        :param activation: functie de activare folosita pentru urmatoarei stari ascunse\n",
    "        \"\"\"\n",
    "        super(VanillaRNNCell, self).__init__()\n",
    "    \n",
    "        self._hidden_size = hidden_size\n",
    "        self._activation = activation()  \n",
    "        self._bias = bias\n",
    "            \n",
    "        # Create the hidden-to-hidden layer\n",
    "        self._linear_hh = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        out = inputs\n",
    "        \n",
    "        ## TODO 1.1: your code here - adaugati pasul recurent la inputuri\n",
    "        ## `inputs` este deja rezultatul proiectiei liniare Wx_t\n",
    "        ## out = ...\n",
    "        \n",
    "        ## TODO end code\n",
    "        \n",
    "        return out, out\n",
    "\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False):\n",
    "        \"\"\"\n",
    "        Creates a vanilla RNN where input-to-hidden is a nn.Linear layer\n",
    "        and hidden-to-output is a nn.Linear layer\n",
    "        \n",
    "        :param input_size: the size of the input to the RNN\n",
    "        :param hidden_size: size of the hidden state of the RNN\n",
    "        :param output_size: size of the output\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.in_to_hidden = nn.Linear(self._input_size, self._hidden_size, bias=self._bias)\n",
    "        self.rnn_cell = VanillaRNNCell(self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_out = nn.Linear(self._hidden_size, self._output_size, bias=self._bias)\n",
    "    \n",
    "    def step(self, input, hidden=None):\n",
    "        input_ = self.in_to_hidden(input)\n",
    "        _, hidden_ = self.rnn_cell(input_, hidden=hidden)\n",
    "        output_ = self.hidden_to_out(hidden_)\n",
    "        \n",
    "        return output_, hidden_\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, force=True, warm_start=10):\n",
    "        batch_size = inputs.size(0)\n",
    "        steps = inputs.size(1)\n",
    "        \n",
    "        outputs = torch.autograd.Variable(torch.zeros(batch_size, steps, self._output_size))\n",
    "        \n",
    "        output_ = None\n",
    "        hidden_ = hidden\n",
    "        input_ = torch.zeros(batch_size, self._input_size)\n",
    "        \n",
    "        ## TODO 1.2 - your code here: propagati secventa de antrenare prin retea\n",
    "        ## Nota: \n",
    "        ##    inputs - tensor de forma (batch_size, seq_len, 1)\n",
    "        ##    self.step primeste intrare de forma input: (batch_size, 1), hidden: (batch_size, hidden_size)\n",
    "        ##    self.step intoarche iesire de forma output: (batch_size, 1), hidden: (batch_size, hidden_size)\n",
    "        ## Daca flag-ul de `force` este True sau daca suntem la primul pas, self.step primeste ca input un element din secventa de intrare\n",
    "        ## Daca flag-ul este False, self.step primeste ca input predictia anterioara a retelei\n",
    "        ## Daca sunt in primii `warm_start` pasi, self.step primeste ca input un element din secventa de intrare\n",
    "        for i in range(steps):\n",
    "            ## input_ = ...\n",
    "            \n",
    "            output_, hidden_ = self.step(input_, hidden_)\n",
    "            outputs[:, i, :] = output_\n",
    "        ## end code\n",
    "        \n",
    "        return outputs, hidden_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.    RNN simplu folosind un modul LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False, batch_first = True, num_layers = 1):\n",
    "        super(LSTMRNN, self).__init__()\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.rnn = LSTM(input_size = input_size, \n",
    "                        proj_size = output_size, \n",
    "                        hidden_size = hidden_size, \n",
    "                        bias=bias,\n",
    "                        num_layers = num_layers, \n",
    "                        batch_first = batch_first)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, hidden=None, force=True, warm_start=10):\n",
    "        batch_size = inputs.size(0)\n",
    "        steps = inputs.size(1)\n",
    "\n",
    "        outputs = torch.autograd.Variable(torch.zeros(batch_size, steps, self._output_size))\n",
    "        \n",
    "        output_ = torch.zeros(batch_size, 1, self._output_size)\n",
    "        hidden_ = None\n",
    "        input_ = torch.zeros(batch_size, 1, self._input_size)\n",
    "\n",
    "        for i in range(steps):\n",
    "            ## TODO 1.2 - your code here: propagati secventa de antrenare prin retea\n",
    "            ## Nota: \n",
    "            ##    inputs - tensor de forma (batch_size, seq_len, 1)\n",
    "            ##    self.step primeste intrare de forma input: (batch_size, 1, 1), hidden: (batch_size, hidden_size)\n",
    "            ##    self.step intoarche iesire de forma output: (batch_size, 1, 1), hidden: (1, batch_size, hidden_size)\n",
    "            ## Daca flag-ul de `force` este True sau daca suntem la primul pas, self.step primeste ca input un element din secventa de intrare\n",
    "            ## Daca flag-ul este False, self.step primeste ca input predictia anterioara a retelei\n",
    "            ## Daca sunt in primii `warm_start` pasi, self.step primeste ca input un element din secventa de intrare\n",
    "            ## input_ = ....\n",
    "            \n",
    "            ## end code\n",
    "            output_, hidden_ = self.rnn(input_, hidden_)\n",
    "            outputs[:, [i], :] = output_\n",
    "        \n",
    "        return outputs, hidden_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antrenarea retelei RNN pentru a prezice o curba sinusoidala\n",
    "\n",
    "Antrenam reteaua pe o curba sinusoidala - predictia urmatoarei valori din sinusoida pe baza valorilor *prezise* anterior.\n",
    "\n",
    "Predictie pentru   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
    "\n",
    "În special, dorim ca rețeaua să prezică următoarea valoare într-o buclă, condiționând predicția de unele valori inițiale (furnizate) și de toate predicțiile ulterioare.\n",
    "\n",
    "Pentru a învăța modelul de predicție, vom folosi **teacher forcing**. Aceasta înseamnă că atunci când antrenam modelul, intrarea la momentul $t$ este secvența reală la momentul $t$, mai degrabă decât rezultatul produs de model la $t-1$. \n",
    "\n",
    "**Când dorim să generăm** date din model, **nu avem acces la succesiunea adevărată**, așa că nu folosim *teacher forcing*. \n",
    "Totuși, în cazul problemei noastre, vom folosi notiunea de **warm start (pornire la cald)**, deoarece avem nevoie de mai mulți pași de timp pentru a prezice următoarea valoare a undei sinusoidale (cel puțin 2, pentru valoarea inițială și pentru pas).\n",
    "\n",
    "Codul de mai jos derulează modelul RNN pe care l-ați definit mai sus, face antrenamentul folosind backprop în timp (BPTT) și trasează datele reale (\"ground truth\"), datele generate în timpul antrenamentului (\"training predictions\") și eșantioanele de model \"generate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code @ cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Running code @ {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "WARM_START = 10  #@param {type:\"integer\"}\n",
    "TEACHER_FORCING = True  #@param {type:\"boolean\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.001  #@param {type:\"number\"}\n",
    "REPORTING_INTERVAL = 200  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 32 #@param {type:\"integer\"}\n",
    "\n",
    "# We create training data, sine wave over [0, 2pi]\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1)\n",
    "y_train = np.sin(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(y_train, unroll_length, batch_size):\n",
    "    y_batch = np.zeros((batch_size, unroll_length, 1))\n",
    "    starts = np.zeros(batch_size, dtype=np.int64)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        start = np.random.choice(range(y_train.shape[0] - UNROLL_LENGTH))\n",
    "        train_sequence = y_train[start : (start + UNROLL_LENGTH)]\n",
    "        \n",
    "        starts[i] = start\n",
    "        y_batch[i, :, :] = train_sequence\n",
    "        \n",
    "    return y_batch, starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = VanillaRNN(hidden_size=HIDDEN_UNITS, bias=True)\n",
    "# net = LSTMRNN(hidden_size=HIDDEN_UNITS, bias=True, num_layers = 1)\n",
    "\n",
    "net.train()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # select a start point in the training set for a sequence of UNROLL_LENGTH\n",
    "    train_batch, starts = create_batch(y_train, UNROLL_LENGTH, BATCH_SIZE)\n",
    "    \n",
    "    train_inputs = torch.from_numpy(train_batch[:,:-1,:]).float().view(BATCH_SIZE, UNROLL_LENGTH-1, 1).to(device)\n",
    "    train_targets = torch.from_numpy(train_batch[:,1:, :]).float().view(BATCH_SIZE, UNROLL_LENGTH-1, 1).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs, hidden = net(train_inputs, hidden=None, force=TEACHER_FORCING, warm_start=WARM_START)\n",
    "    \n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % REPORTING_INTERVAL == REPORTING_INTERVAL - 1:\n",
    "        # let's see how well we do on predictions for the whole sequence\n",
    "        avg_loss = running_loss / REPORTING_INTERVAL\n",
    "        running_loss = 0\n",
    "        \n",
    "        report_sequence = torch.from_numpy(y_train[:-1]).float().view(1, len(y_train)-1, 1).to(device)\n",
    "        report_targets = torch.from_numpy(y_train[1:]).float().view(1, len(y_train)-1, 1).to(device)\n",
    "        report_output, report_hidden = net(report_sequence, hidden=None, force=False, warm_start=WARM_START)\n",
    "        \n",
    "        report_loss = criterion(report_output, report_targets)\n",
    "        print('[%d] avg_loss: %.5f, report_loss: %.5f, ' % (iteration + 1, avg_loss, report_loss.item()))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Training Loss %.5f;  Sampling loss %.5f; Iteration %d' % (avg_loss, report_loss.item(), iteration))\n",
    "        \n",
    "        plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
    "               linestyle=\":\", lw=6)\n",
    "        plt.plot(range(starts[0], starts[0]+UNROLL_LENGTH-1), outputs[0, :, :].data.numpy().ravel(), c='gold',\n",
    "               label='Train prediction', lw=5, marker=\"o\", markersize=5,\n",
    "               alpha=0.7)\n",
    "        plt.plot(report_output.data.numpy().ravel(), c='r', label='Generated', lw=4, alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jCR9YGaI7my"
   },
   "source": [
    "# TASK 3: Evaluarea influentei hiper-parametrilor\n",
    "\n",
    "Testati pe baza codului de mai sus urmatoarele configuratii:\n",
    "  * UNROLL_LENGTH: 3, 10, 30, 62\n",
    "  * TEACHER_FORCING: True, False\n",
    "  * WARM_START: 2, 5, 10\n",
    "  \n",
    "Raspundeti la urmatoarele intrebari:\n",
    "  1. Care este influenta TEACHER_FORCING-ului? In ce situatii ar trebui evitat / redus?\n",
    "  2. Care este influenta UNROLL_LENGTH-ului (lungimea secventei de antrenare)? Ce se poate intampla daca UNROLL_LENGTH este foarte mic?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwojS8kx0QyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEML2019_RNN_full.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
