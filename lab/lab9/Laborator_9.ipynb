{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab9/Laborator_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmaHAc_KHG_5"
   },
   "source": [
    "# Rețele neurale pentru clasificare imaginilor\n",
    "\n",
    "_Tudor Berariu, 2018_ (tudor.berariu@gmail.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKCKTWvsuJuF"
   },
   "source": [
    "În cadrul acestui laborator veți implementa o rețea neurală cu arhitectură convoluțională pentru clasificarea imaginilor.\n",
    "\n",
    "Rețeaua va fi compusă din straturi convoluționale, straturi de Pooling pentru reducerea dimensionalității și activări de tip ReLU.\n",
    "Rețeaua va avea straturi lineare la final și un strat softmax înainte de ieșiri. \n",
    "\n",
    "Funcția de cost folosită este negative log likelihood. Pentru optimizarea acesteia se va folosi SGD (stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Resurse teoretice\n",
    "\n",
    "* Citiți de aici despre straturile convoluțional și max pooling:\n",
    "http://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "* Citiți de aici despre cum se implementează eficient pașii de convoluție folosind funcția im2col din numpy:\n",
    "https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgNIWhzoHOIz"
   },
   "source": [
    "## 1. Setul de date MNIST\n",
    "\n",
    "Setul de date MNIST este compus din imagini de 28x28 pixeli reprezentând una dintre cele zece cifre 0-9.\n",
    "\n",
    "Decomentați mai jos comanda `!pip install mnist` pentru a instala pachetul `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQiqJyO7E7Ek"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: numpy in /home/alex/python-envs/venv-py36/lib/python3.6/site-packages (from mnist) (1.14.5)\n",
      "Installing collected packages: mnist\n",
      "Successfully installed mnist-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mnist\n",
    "\n",
    "import mnist\n",
    "train_imgs = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_imgs = mnist.test_images()\n",
    "test_labels  = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElGfqnPzuJuO"
   },
   "source": [
    "### Exemple din setul de date MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeftJ_CpE7Eu"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avJh9wQquJuU"
   },
   "outputs": [],
   "source": [
    "idxs = np.random.randint(0, len(train_imgs), 15)\n",
    "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
    "plt.imshow(imgs)\n",
    "print(\"Labels:\", train_labels[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFP9QnUJuJuY"
   },
   "source": [
    "### Standardizarea datelor\n",
    "\n",
    "Datele de intrare (imaginile) vor fi rescalate pentru a avea media zero și deviația standard 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWmt4XVxuJuZ"
   },
   "outputs": [],
   "source": [
    "mean, std  = train_imgs.mean(), train_imgs.std()\n",
    "train_imgs = (train_imgs - mean) / std\n",
    "test_imgs = (test_imgs - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgERUA07IuSr"
   },
   "source": [
    "## 2. Construirea unei rețele de tip feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE-V6sONuJud"
   },
   "source": [
    "### Notații\n",
    "  - dimensiunea datelor de intrare este $D = 28 * 28 = 784$, iar dimensiunea ieșirilor rețelei este $K=10$ (numărul de clase)\n",
    "  - rețeaua neurală va avea $L$ straturi\n",
    "  - $B$ va reprezenta dimensiunea batch-ului (numărul de exemple trecute în același timp prin rețea)\n",
    "  - Vom nota cu ${\\bf X} \\in {\\mathbb R}^{B \\times D}$ un batch de intrări $\\left\\lbrace {\\bf x}_0, {\\bf x}_1, \\dots {\\bf x}_B \\right\\rbrace$ și similar ${\\bf Y} \\in {\\mathbb R}^{B \\times K}$\n",
    "  - ${\\bf x}^{(l)}$ reprezintă intrările stratului $l$ (${\\bf x}^{(0)}$ va fi o imagine precum cele din setul MNIST de dimensiune $D$)\n",
    "  - ${\\bf y}^{(l)}$ reprezintă ieșirile stratului $l$ (${\\bf y}^{(L-1)}$ reprezintă ieșirile rețelei)\n",
    "  - ${\\bf \\theta}^{(l)}$ reprezintă parametrii stratului $l$\n",
    "  - ${\\cal L}$ reprezintă funcția de cost ( _negative log likelihood_ )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YTu4tS8uJue"
   },
   "source": [
    "### Straturile rețelei\n",
    "\n",
    "Unele straturi au parametri ce trebuie optimizați în timpul antrenării. Vom nota parametrii stratului $l$ cu $\\bf{\\theta}^{(l)}$.\n",
    "Fiecare strat pe care îl veți implementa va avea trei metode:\n",
    " - `forward` calculează și întoarce ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf \\theta}^{(l)}\\right)$\n",
    " - `backward` primește $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, reține intern $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}^{(l)}}$ și întoarce $\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l)}}$\n",
    " - `update` modifică parametrii locali ${\\bf \\theta}^{(l)}$ folosing gradientul stocat $\\frac{\\partial{\\cal L}}{\\partial{\\bf \\theta}^{(l)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW206j3euJuf"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass  # If a layer has no parameters, then this function does nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIT6K4IduJuk"
   },
   "source": [
    "### Rețeaua neurală\n",
    "\n",
    "  * în faza `forward` ieșirile stratului $l$ devin intrările stratului $l+1$: ${\\bf x}^{(l+1)} = {\\bf y}^{(l)}$\n",
    "  * în faza `backward` gradientul în raport cu intrările stratului $l+1$ devine gradientul în raport cu ieșirile stratului $l$: $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}=\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l+1)}}$\n",
    "  \n",
    "Completați metoda `backward` din clasa `FeedForwardNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTn-g3KAuJul"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
    "        self._inputs = []\n",
    "        for layer in self.layers:\n",
    "            if train:\n",
    "                self._inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
    "        # TODO <0> : Compute the backward phase\n",
    "        raise NotImplementedError\n",
    "        del self._inputs\n",
    "    \n",
    "    def update(self, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            layer.update(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyQLVM4quJup"
   },
   "source": [
    "### Stratul linear\n",
    "\n",
    "Un strat linear cu $M$ intrări și $N$ ieșiri are parametrii $\\theta = \\left( {\\bf W}, {\\bf b} \\right)$ unde ${\\bf W} \\in \\mathbb{R}^{M \\times N}$ și ${\\bf b} \\in \\mathbb{R}^{N}$.\n",
    "\n",
    "Pentru un singur exemlu ${\\bf x} \\in {\\mathbb R}^{M}$:\n",
    "$$ {\\bf y} = {\\bf x}^{\\intercal}{\\bf W} + {\\bf b} $$\n",
    "\n",
    "Implementați metoda `forward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și întoarce ieșirile corespunzătoare: $Y \\in {\\mathbb R}^{B\\times N}$.\n",
    "\n",
    "Implementați metoda `backward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și gradientul în raport cu ieșirile $\\frac{\\partial {\\cal L}}{\\partial {\\bf Y}}$ și realizează două lucruri:\n",
    "  - calculează și salvează intern gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}}$\n",
    "  - calculează și întoarce gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf X}}$\n",
    "  \n",
    "Implementați strategia de optimizare SGD cu _momentum_ (în metoda `update`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S47ZsyKdE7FF"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, insize: int, outsize: int) -> None:\n",
    "        bound = np.sqrt(6. / insize)\n",
    "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
    "        self.bias = np.zeros((outsize,))\n",
    "        \n",
    "        self.dweight = np.zeros_like(self.weight)\n",
    "        self.dbias = np.zeros_like(self.bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # compute the output of a linear layer\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # compute dweight, dbias and  return dx\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, mode='SGD', lr=0.001, mu=0.9):\n",
    "        if mode == 'SGD':\n",
    "            self.weight -= lr * self.dweight\n",
    "            self.bias -= lr * self.dbias\n",
    "        elif mode == 'momentum':\n",
    "            # implement momentum update\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('mode should be SGD or momentum, not ' + str(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgfHlVgDuJut"
   },
   "source": [
    "### The Rectified Linear Unit\n",
    "\n",
    "Stratul ReLU aplică următoare următoare transformare neliniară element cu element:\n",
    "$$y = \\max\\left(x, 0\\right)$$\n",
    "\n",
    "Implementați metodele `forward` și `backward` pentru un strat de activare ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOR1DJiwE7FJ"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <3> : Compute the output of a rectified linear unit\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <4> : Compute the gradient w.r.t. x\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Convolutional Layer\n",
    "\n",
    "**[Cerințele 9.1 - 9.2] (4p)** Implementați metodele `forward` și `backward` pentru un strat convolutional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Im2Col auxiliries from [CS231n assignment](https://github.com/huyouare/CS231n/blob/master/assignment2/cs231n/im2col.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "  # First figure out what the size of the output should be\n",
    "  N, C, H, W = x_shape\n",
    "  assert (H + 2 * padding - field_height) % stride == 0\n",
    "  assert (W + 2 * padding - field_height) % stride == 0\n",
    "  out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "  out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "  i0 = np.repeat(np.arange(field_height), field_width)\n",
    "  i0 = np.tile(i0, C)\n",
    "  i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "  j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "  j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "  i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "  j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "  k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "  return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "  \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "  # Zero-pad the input\n",
    "  p = padding\n",
    "  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "  k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "\n",
    "  cols = x_padded[:, k, i, j]\n",
    "  C = x.shape[1]\n",
    "  cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "  return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "  \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "  N, C, H, W = x_shape\n",
    "  H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "  x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "  k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "  cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "  cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "  np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "  if padding == 0:\n",
    "    return x_padded\n",
    "  return x_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, inputs_depth: int, inputs_height: int, inputs_width: int, outputs_depth: int, k: int, stride: int):\n",
    "        # Number of inputs, number of outputs, filter size, stride\n",
    "\n",
    "        self.inputs_depth = inputs_depth\n",
    "        self.inputs_height = inputs_height\n",
    "        self.inputs_width = inputs_width\n",
    "\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "\n",
    "        self.outputs_depth = outputs_depth\n",
    "        self.outputs_height = int((self.inputs_height - self.k) / self.stride + 1)\n",
    "        self.outputs_width = int((self.inputs_width - self.k) / self.stride + 1)\n",
    "\n",
    "        # Layer's parameters\n",
    "        self.weights = np.random.normal(\n",
    "            0,\n",
    "            np.sqrt(2.0 / float(self.outputs_depth + self.inputs_depth + self.k + self.k)),\n",
    "            (self.outputs_depth, self.inputs_depth, self.k, self.k)\n",
    "        )\n",
    "        self.biases = np.random.normal(\n",
    "            0,\n",
    "            np.sqrt(2.0 / float(self.outputs_depth + 1)),\n",
    "            (self.outputs_depth, 1)\n",
    "        )\n",
    "\n",
    "        # Gradients\n",
    "        self.g_weights = np.zeros(self.weights.shape)\n",
    "        self.g_biases = np.zeros(self.biases.shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "        assert (dd, hh, ww == self.inputs_depth, self.inputs_height, self.inputs_width)\n",
    "\n",
    "        # Computed values\n",
    "        outputs = np.zeros((batch_size, self.outputs_depth, self.outputs_height, self.outputs_width))\n",
    "\n",
    "        # TODO (9.1)\n",
    "        # -> compute outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, inputs: np.ndarray, output_errors: np.ndarray) -> np.ndarray:\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "        assert (output_errors.shape == batch_size, self.outputs_depth, self.outputs_height, self.outputs_width)\n",
    "\n",
    "        # TODO (9.2.i)\n",
    "        # Compute the gradients w.r.t. the bias terms (self.g_biases)\n",
    "        # self.g_biases = ...\n",
    "        \n",
    "        # TODO (9.2.ii)\n",
    "        # Compute the gradients w.r.t. the weights (self.g_weights)\n",
    "        # self.g_weights = ...\n",
    "        #\n",
    "        # TODO (9.2.iii)\n",
    "        # Compute and return the gradients w.r.t the inputs of this layer\n",
    "        # return these gradients\n",
    "        \n",
    "        dx = np.zeros(inputs.shape)\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "    def update_parameters(self, mode='SGD', lr=0.001, mu=0.9):\n",
    "        if mode == 'SGD':\n",
    "            self.weights -= lr * self.g_weights\n",
    "            self.biases -= lr * self.g_biases\n",
    "        elif mode == 'momentum':\n",
    "            # implement/reuse momentum update from Lab 8\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError('mode should be SGD or momentum, not ' + str(mode))\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[C ((%s, %s, %s) -> (%s, %s ) -> (%s, %s, %s)]\" % (\n",
    "            self.inputs_depth, self.inputs_height, self.inputs_width, self.k, self.stride, self.outputs_depth,\n",
    "            self.outputs_height, self.outputs_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Max Pooling Layer\n",
    "\n",
    "**[Cerința 9.3] (2p)** Implementați metodele `forward` și `backward` pentru un strat de tip Max Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer(Layer):\n",
    "\n",
    "    def __init__(self, size: int = 2, stride: int = 2):\n",
    "        # Dimensions: stride\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "        # indexes of max activations\n",
    "        self.switches = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, inputs, output_errors):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[MP (%s x %s)]\" % (self.size, self.stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linearization Layer\n",
    "\n",
    "**[Cerința 9.4] (2p)** Implementați metodele `forward` și `backward` pentru un strat de tip Linearization.\n",
    "\n",
    "**Nota** Acesta este un strat care rearanjeaza datele dintr-un volum (numar canale x inaltime x latime) intr-un singur vector; (util la trecerea de la volume de imagini la straturi complet conectate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearizeLayer(Layer):\n",
    "\n",
    "    def __init__(self, depth: int, height: int, width: int):\n",
    "        # Dimensions: depth, height, width\n",
    "        self.depth = depth\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs are of shape (batch_size, depth, height, width)\n",
    "        # outputs are of shape (batch_size, depth x height x width)\n",
    "        \n",
    "        # TODO 1\n",
    "        # Reshape inputs- transform volume to column\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, inputs: np.ndarray, output_errors: np.ndarray) -> np.ndarray:\n",
    "        # unused argument - inputs\n",
    "        # output_errors of shape (batch_size, depth x height x width)\n",
    "\n",
    "        # TODO 1\n",
    "        # Reshape gradients - transform column to volume\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[Lin ((%s, %s, %s) -> %s)]\" % (self.depth, self.height, self.width, self.depth * self.height * self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NrWBTmbI9gW"
   },
   "source": [
    "## 3. Funcția de cost\n",
    "\n",
    "Funcția de cost pe care o vom folosi este _cross entropy_ care combină un _softmax_ și un cost _negative log-likelihood_. (Matematica la tablă)\n",
    "\n",
    "Dacă ${\\bf y}$ reprezintă ieșrile rețelei pentru o intrare ${\\bf x}$, atunci ${\\bf y}$ va avea o dimensiune egală cu numărul de clase $K$. Atunci probabilitatea (prezisă de rețea) ca exemplul ${\\bf x}$ să aparțină clasei $k$ va fi $p_k$:\n",
    "$$\\begin{align}\n",
    "p_k &= \\frac{e^{y_k}}{\\sum_j e^{y_j}} & & \\text{softmax} \\\\\n",
    "{\\cal L} &= -\\log p_t & & \\text{negative log-likelihood}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Pentru un batch de dimensiune $B$ se va face media costurilor corespunzătare fiecărui exemplu ($p_k$ este o funcție de ${\\bf x}$ și ${\\bf \\theta}$):\n",
    "\n",
    "$$ {\\cal L} = \\frac{1}{B} \\sum_{({\\bf x}, {\\bf t}) \\in Batch} -\\log p_t \\left({\\bf x}, \\theta\\right) $$\n",
    "\n",
    "Implementați metodele `forward` și `backward` pentru un funcția de cost _cross-entropy_ (o vom privi ca pe un strat suplimentar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDXiDEu8E7FW"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
    "        # Compute the negative log likelihood\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        # Compute dl/dy\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uz9qM5eHJLNw"
   },
   "source": [
    "### Acuratețea\n",
    "\n",
    "Calculați acuratețea predicțiilor ${\\bf y}$ în raport cu clasele corecte ${\\bf t}$ (rația exemplelor pentru care clasa corectă a avut probabilitatea prezisă maximă)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nYfVCBSE7Fe"
   },
   "outputs": [],
   "source": [
    "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
    "    # Compute accuracy\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIhtzd2gJQF2"
   },
   "source": [
    "## 4. Antrenarea rețelei neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cerinta 9.5] (2p) - Crearea si antrenarea unei arhitecturi de retea convolutionala similara cu LeNet\n",
    "\n",
    "Implementati [arhitectura LeNet](https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/) \n",
    "  - Conv(1, 28, 28, 20, 5, 1), Relu, MaxPool(2, 2), Conv(20, 12, 12, 50, 5, 1), Relu, MaxPool(2,2), Linearize(50, 4, 4), Linear(800, 500), Relu, Linear(500, 10), Softmax\n",
    "  \n",
    "**Adaptati codul de antrenare din laboratorul precedent pentru a antrena reteaua.**\n",
    "\n",
    "**Comparati rezultatele obtinute cu cele ale retelei MLP antrenate data trecuta.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9.5: antrenare retea LeNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7zGgHlduJvA"
   },
   "source": [
    "## Teste\n",
    "\n",
    "Executați ```test0() and test16() and test7()``` pentru a rula testele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YaLsPBfuJvB"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm \n",
    "\n",
    "def close_enough(arr1, arr2, max_err = 0.0000001):\n",
    "    assert(arr1.shape == arr2.shape)\n",
    "    return norm(arr1.reshape(arr1.size) - arr2.reshape(arr2.size)) < max_err\n",
    "\n",
    "def test0():\n",
    "    fakex = [np.random.randn(128, n) for n in [20, 40, 30, 10]]\n",
    "\n",
    "    class DummyLayer:\n",
    "        def __init__(self, idx):\n",
    "            self.idx = idx\n",
    "\n",
    "        def forward(self, x):\n",
    "            return fakex[self.idx + 1]\n",
    "\n",
    "        def backward(self, x, dldy):\n",
    "            if not np.allclose(x, fakex[self.idx]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            if not np.allclose(dldy, -fakex[self.idx+1]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            return -x\n",
    "\n",
    "    try:\n",
    "        net = FeedForwardNetwork([DummyLayer(i) for i in range(3)])\n",
    "        net.forward(fakex[0])\n",
    "        net.backward(-fakex[-1])\n",
    "        print(\"Cerința 0 rezolvată corect!\")\n",
    "        return True\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 0 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 0 are erori.\")\n",
    "        \n",
    "    return False\n",
    "        \n",
    "def test16():\n",
    "    __x = np.array([[-3.0731, -1.9081, -0.7283, -0.0757, -0.7577],\n",
    "                    [ 2.4041, -1.1506, -0.5924,  1.3016,  1.0882],\n",
    "                    [-0.5254,  0.3519, -0.9633, -2.7393, -0.9745]])\n",
    "    __w = np.array([[ 1.3214, -0.5886, -0.0351,  1.2084,  1.2661, -0.9979, -0.1172],\n",
    "                    [-0.4022,  0.1168,  0.9020, -2.0098, -0.5409, -0.3876, -0.1719],\n",
    "                    [-1.1125, -0.5556,  0.8843,  0.6995,  0.4929,  0.7523,  0.1832],\n",
    "                    [ 0.2267,  0.6757,  1.1286, -0.3218,  1.6934, -0.1782, -0.3467],\n",
    "                    [-0.6062,  0.4426,  0.5090,  0.4772, -0.5721,  0.8658, -0.5999]])\n",
    "    __b = np.array([ 0.3335,  0.5051, -0.1393,  1.2116,  1.7836, -0.6597,  0.3553])\n",
    "    __y = np.array([[-1.70746622, 2.10919555, -2.8676804, 0.48630531, -1.1288499, 1.95609904, 1.39083457],\n",
    "                    [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, -2.34822291, -0.94127596],\n",
    "                    [0.5391161, -0.89159687, -4.24288533, -0.38789499, -3.62798139, -1.35206921, 1.71422657]])\n",
    "    \n",
    "    __dy = np.array([[ 1.5555, -0.8978, -0.2917, -0.3868, -0.8257, -0.3491, -0.8658],\n",
    "                     [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0.4794,  0.8565],\n",
    "                     [-0.1552, -1.6319,  1.7642,  1.0503,  0.1035 , -0.7186, -0.9782]])\n",
    "    __dx = np.array([[ 1.53113221,  0.51455541, -2.588423,   -1.49460989, -0.98384103],\n",
    "                     [ 0.41215308,  0.46469672, -0.59552791,  3.04147235, -0.08763244],\n",
    "                     [ 2.92549149, -0.25707023,  2.70531668,  1.15769427,  0.67643021]])\n",
    "    __dw = np.array(\n",
    "        [[-2.01905511,  7.20190418,  2.2752849,   0.00865613,  3.89837344,  2.60289719, 5.23374791],\n",
    "         [-4.30512319, -0.57717827,  0.07387429,  1.40830543,  0.9345816,  -0.13835527, 0.3223155 ],\n",
    "         [-1.64365553,  1.34237165, -2.05517959, -0.57525343,  0.15290988,  0.66248035, 1.0654716 ],\n",
    "         [ 1.75815137,  6.47943337, -3.56222681, -3.18791411,  0.54523986,  2.61887489, 3.85994472],\n",
    "         [ 0.18554777,  3.89349109, -0.45449919, -1.01478565,  1.16539548,  1.48647185, 2.54131586]]\n",
    "    )\n",
    "    __db = np.array([ 2.5149, -1.0383,  2.4316,  0.4022, -0.1335, -0.5883, -0.9875])\n",
    "    \n",
    "    __y_relu = np.array([[0, 2.10919555, 0, 0.48630531, 0, 1.95609904, 1.39083457],\n",
    "                         [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, 0, 0],\n",
    "                         [0.5391161, 0, 0, 0, 0, 0, 1.71422657]])\n",
    "    __drelu = np.array([[0, -0.8978, 0, -0.3868, 0, -0.3491, -0.8658],\n",
    "                        [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0,  0],\n",
    "                        [-0.1552, 0,  0,  0,  0 , 0, -0.9782]])\n",
    "    \n",
    "    __t = np.array([3, 1, 2])\n",
    "    __dl_dy = np.array(\n",
    "        [[ 2.80870645e-03,  1.27661957e-01,  8.80302096e-04, -3.08142112e-01,\n",
    "           5.00952130e-03,  1.09539948e-01,  6.22416775e-02],\n",
    "         [ 1.73238217e-02, -3.32870086e-01,  3.07917841e-04,  1.09927743e-01,\n",
    "           2.05192672e-01,  2.31991342e-05,  9.47329526e-05],\n",
    "         [ 6.60308812e-02,  1.57905168e-02, -3.32780047e-01,  2.61307149e-02,\n",
    "           1.02329216e-03,  9.96358772e-03,  2.13841054e-01]]\n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "        lin = Linear(5, 7)\n",
    "        lin.weight = __w.copy()\n",
    "        lin.bias = __b.copy()\n",
    "        y = lin.forward(__x.copy())\n",
    "        if not np.allclose(y, __y):\n",
    "            raise Exception(\"Ieșiri greșite\")\n",
    "        print(\"Cerința 1 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 1 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 1 are erori.\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        dx = lin.backward(__x.copy(), __dy.copy())\n",
    "        if not np.allclose(dx, __dx):\n",
    "            raise ValueError(\"dL/dx greșit\")\n",
    "        if not np.allclose(lin.dweight, __dw):\n",
    "            raise ValueError(\"dL/dw greșit\")\n",
    "        if not np.allclose(lin.dbias, __db):\n",
    "            raise ValueError(\"dL/db greșit\")\n",
    "        print(\"Cerința 2 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 2 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 2 are erori.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        y_relu = relu.forward(__y.copy())\n",
    "        if not np.allclose(y_relu, __y_relu):\n",
    "            raise ValueError(\"ReLU(x) greșit\")\n",
    "        print(\"Cerința 3 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 3 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 3 are erori.\")\n",
    "        return False\n",
    "            \n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        drelu = relu.backward(__y.copy(), __dy.copy())\n",
    "        if not np.allclose(drelu, __drelu):\n",
    "            raise ValueError(\"ReLU.backward greșit\")\n",
    "        print(\"Cerința 4 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 4 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 4 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        loss = ce.forward(__y.copy(), __t.copy())\n",
    "        if np.abs(loss - 5.1874357237332545) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită nll: {loss:f} în loc de 5.1874357237332545\")\n",
    "        print(\"Cerința 5 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 5 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 5 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        dl_dy = ce.backward(__y.copy(), __t.copy())\n",
    "        if not np.allclose(dl_dy, __dl_dy) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită pentru dNLL/dy\")\n",
    "        print(\"Cerința 6 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 6 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 6 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test7():  # Acuratețea\n",
    "    y = np.array([[ 0.6460014 , -0.05876393, -1.36496105, -0.07057596,  0.54938383],\n",
    "                  [-0.8033942 , -0.51753041,  0.92278036, -1.66303585, -0.36537512],\n",
    "                  [-1.3710599 ,  0.65598193, -0.75527154,  1.21609284,  0.08284123],\n",
    "                  [-1.24696857,  0.32676634,  0.09572539,  1.38316398, -0.14110726],\n",
    "                  [-2.01698315,  2.06123375, -1.68003675,  0.0504592 ,  0.04427597],\n",
    "                  [-0.8893451 ,  1.74695148, -0.29394473,  0.74203068, -0.75185261],\n",
    "                  [ 1.34126333, -0.5272606 ,  1.46458319,  1.59529987,  1.86884676],\n",
    "                  [-0.58987297,  1.10900165, -0.71208103,  0.20478154, -1.26693567],\n",
    "                  [-2.17730677, -1.36147532, -1.49679182,  0.24812177, -0.13368035],\n",
    "                  [-0.48730599,  1.31710647,  0.41765538,  1.19869192, -0.05301611],\n",
    "                  [-0.10655224, -0.21174034,  1.31548647, -0.57990281,  0.85868472],\n",
    "                  [-0.32055613, -2.17817118, -0.28488692,  1.62977524,  0.25150929],\n",
    "                  [ 0.07704727,  1.67710047,  1.83368441, -0.45456845, -0.74474969]])\n",
    "    t = np.array([0, 2, 3, 3, 1, 0, 1, 1, 2, 1, 2, 3, 2])\n",
    "    try:\n",
    "        acc = accuracy(y, t)\n",
    "        if np.abs(acc - 0.7692307692307693) > 1e-7:\n",
    "            raise ValueError(f\"{acc:f} != 10/13\")\n",
    "        print(f\"Cerința 7 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 7 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 7 are erori.\")\n",
    "\n",
    "\n",
    "def test_convolutional_layer():\n",
    "\n",
    "    np.random.seed(0)\n",
    "    l = ConvolutionalLayer(2, 3, 4, 3, 2, 1)\n",
    "\n",
    "    l.weights = np.random.rand(3, 2, 2, 2)\n",
    "\n",
    "    l.biases = np.random.rand(3, 1)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[[[ 34.55043437, 38.95942899, 43.36842361],\n",
    "                        [ 52.18641284, 56.59540746, 61.00440208]],\n",
    "    [[ 30.72457988, 34.08923073, 37.45388158],\n",
    "     [ 44.18318328, 47.54783413, 50.91248498]],\n",
    "     [[ 28.2244684, 31.30220961, 34.37995083],\n",
    "      [ 40.53543326, 43.61317448, 46.69091569]]]])\n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "    output_err = np.random.rand(1, 3, 2, 3)\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "    print(l.g_biases)\n",
    "#    print(l.g_weights)\n",
    "#    print(g)\n",
    "\n",
    "    print(\"    i. testing gradients w.r.t. the bias terms...\")\n",
    "    gbias_target =  np.array([[ 2.4595299 ],\n",
    "                              [ 3.86207926],\n",
    "                              [ 1.17504241]])\n",
    "\n",
    "    print(l.g_biases.shape)\n",
    "    print(gbias_target.shape)\n",
    "\n",
    "    assert (l.g_biases.shape == gbias_target.shape), \"Wrong size\"\n",
    "    assert close_enough(l.g_biases, gbias_target), \"Wrong values\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"   ii. testing gradients w.r.t. the weights...\")\n",
    "    gweights_target = np.array(\n",
    "            [[[[ 12.19071134, 14.65024124],\n",
    "               [ 22.02883093, 24.48836083]],\n",
    "    [[ 41.70507011, 44.1646    ],\n",
    "     [ 51.54318969, 54.00271959]]],\n",
    "\n",
    "    [[[ 17.14269456, 21.00477382],\n",
    "      [ 32.59101161, 36.45309087]],\n",
    "\n",
    "  [[ 63.4876457 , 67.34972496],\n",
    "   [ 78.93596275, 82.79804201]]],\n",
    "\n",
    " [[[  5.38434096,  6.55938337],\n",
    "   [ 10.08451061, 11.25955302]],\n",
    "\n",
    "  [[ 19.4848499 , 20.65989231],\n",
    "   [ 24.18501955, 25.36006196]]]]\n",
    "    )\n",
    "\n",
    "    assert (l.g_weights.shape == gweights_target.shape), \"Wrong size\"\n",
    "    assert close_enough(l.g_weights, gweights_target), \"Wrong values\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "\n",
    "def test_max_pooling_layer():\n",
    "\n",
    "    l = MaxPoolingLayer(2)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8]],\n",
    "                  [[9, 10, 11, 12], [13, 14, 15, 16]],\n",
    "                  [[17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[[[6, 8]],\n",
    "                       [[14, 16]],\n",
    "                       [[22, 24]]]])\n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "\n",
    "    output_err = output\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "    print(g)\n",
    "\n",
    "\n",
    "    print(\"Testing gradients\")\n",
    "    in_target = np.array([[[[0, 0, 0, 0], [0, 6, 0, 8]],\n",
    "                          [[0, 0, 0, 0], [0, 14, 0, 16]],\n",
    "                          [[0, 0, 0, 0], [0, 22, 0, 24]]]])\n",
    "\n",
    "    assert (g.shape == in_target.shape), \"Wrong size\"\n",
    "    assert close_enough(g, in_target), \"Wrong values in gradients\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"Backward computation implemented ok!\")\n",
    "    \n",
    "\n",
    "def test_linearize_layer():\n",
    "\n",
    "    l = LinearizeLayer(2, 3, 4)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]])\n",
    "    target = target.reshape(1, len(target))\n",
    "    \n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "    output_err = output\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "\n",
    "    print(\"Testing gradients\")\n",
    "    in_target = x\n",
    "\n",
    "    assert (g.shape == in_target.shape), \"Wrong size\"\n",
    "    assert close_enough(g, in_target), \"Wrong values in gradients\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"Backward computation implemented ok!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWQ2e7C4uJvD"
   },
   "outputs": [],
   "source": [
    "#test0() and test16() and test7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_convolutional_layer() and test_max_pooling_layer() and test_linearize_layer()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Laborator 8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
