{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab4/Laborator_4-Skel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting\n",
    "=======\n",
    "\n",
    "* Mihai Trăscău, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scopul Laboratorului\n",
    "Acest laborator revizitează tematica învățării cu ansamble ( _ensemble learning_ ), punându-se accentul pe concepte de **boosting**.\n",
    "\n",
    "**Boosting** este un meta-algoritm care se bazează la fel ca și celelalte metode de _ensemble learning_ pe construirea unui clasificator _puternic_ utilizând o serie de clasificatori _slabi_ (eng. weak/base learner) care „decid împreună”.  \n",
    "Cealaltă clasa principală de metode de _ensemble_ este **bagging**, al cărui algoritm exponent este *RandomForest*, care a fost studiat anterior.\n",
    "\n",
    "În general, în boosting este permisă utilizarea oricărui tip de clasificator slab. \n",
    "Spre deosebire de algoritmii de tip bagging (unde instanțele de clasificatori slabi sunt antrenați independent), în boosting fiecare instanță de weak learner este antrenată și adăugată ansamblului în mod iterativ (a se vedea si figura de mai jos).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/1276/1*8T4HEjzHto_V8PrEFLkd9A.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noțiuni teoretice despre Boosting\n",
    "### 2.1. AdaBoost\n",
    "_AdaBoost_ este printre primele metode de boosting propuse [1][2] care a produs rezultate bune. Se bazează pe antrenarea unui ansamblu de clasificatori pentru care ipoteza ansamblului (predicția), $H_{T}(x)$, este dată de formula\n",
    "$$\n",
    "H_{T}(x) = \\sum_{t=1}^{T}\\rho_{t}h_{t}(x)\n",
    "$$\n",
    "unde $x$ reprezintă datele de intrare, $h_{t}(x)$ este ipoteza fiecărui clasificator slab din ansamblu iar $\\rho_{t}$ este ponderea cu care acesta „participă” la decizie.\n",
    "\n",
    "La ficare pas $t$ de antrenare este adăugat un nou clasificator slab (care produce ipoteza $h_{t}(x)$). Pentru a informa mai bine fiecare clasificator slab care va fi adăugat ulterior, AdaBoost se bazează pe adăugarea de ponderi pentru elementele din setul de date. După fiecare pas, setul de date de antrenare este reponderat astfel încât exemplele „dificile” de până la pasul $t$ să devină „prioritare” spre a fi clasificate corect de la pasul $t+1$.\n",
    "\n",
    "Altfel spus, la fiecare pas este ales clasificatorul cu ipoteza $h_{t}$ care minimizează eroarea totală ponderată\n",
    "$$\n",
    "q_{i}^{(T-1)} = \\sum_{y_{i}\\neq h_{t}(x_{i})}e^{-y_{i}H_{T-1}(x_{i})}\n",
    "$$\n",
    "pentru care putem calcula rate de eroare\n",
    "$$\n",
    "\\epsilon_{t} = \\frac{\\sum_{y_{i}\\neq h_{t}(x_{i})} q_{i}^{(T-1)}}{\\sum_{i=1}^{N} q_{i}^{(T-1)}}\n",
    "$$\n",
    "Astfel, putem determina ponderea noului clasificator ca fiind\n",
    "$$\n",
    "\\rho_{t} = \\frac{1}{2}ln(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}})\n",
    "$$\n",
    "ceea ce ne permite să formulăm $H_{T} = H_{T-1} + \\rho_{t}h_{t}$\n",
    "### 2.2. Gradient Boosting\n",
    "#### A. Regresie\n",
    "La fel ca AdaBoost și metoda Gradient Boosting antrenează clasificatori (regresori) slabi pe care îi adaugă la ansamblu. Însă, spre deosebire de AdaBoost exemplele din setul de antrenare pentru care sunt prezise valori greșite  nu primesc mai multă „importanță”. În schimb, se bazează pe calcularea _valorilor reziduale_ pentru fiecare exemplu în parte. \n",
    "\n",
    "Să presupunem că la pasul $t$ avem ipoteza $H_{T-1}$ pe care vrem să o îmbunătățim adăugând un nou clasificator (regresor). Astfel, predicția $y_{i}$ pentru fiecare exemplu dat $x_{i}$ va deveni\n",
    "$$\n",
    "H_{T-1}(x_{i}) + h_{t}(x_{i}) = y_{i}\n",
    "$$\n",
    "sau echivalent, considerăm că trebuie să găsim acel clasificator (regresor) pentru care\n",
    "$$\n",
    "h_{t}(x_{i}) = y_{i} - H_{T-1}\n",
    "$$\n",
    "Cum găsirea exactă a lui $h_{t}$ nu este posibilă, vom calcula o aproximare. Astfel, vom genera perechile $(x_{i}, y_{i} - F_{T-1}(x_{i}))$. Setul format din perechile de date de intrare și valori reziduale va fi utilizat pentru antrenarea unui nou clasificator (regresor), care prin predicția sa încearcă minimizarea erorii globale de predicție.\n",
    "#### B. Clasificare (multi-clasă)\n",
    "Gradient Boosting aplicat pe probleme de clasificare aduce o serie de modificări. În primul rând, în loc de o valoare (regresie) ansamblul trebuie să prezică clasa $C_{k}$ din care face parte exemplul $x_{i}$. Pentru a realiza acest lucru vom genera $N$ ipoteze corespunzătoare probabilităților de apartenență la fiecare din cele $N$ clase în parte\n",
    "$$\n",
    "P(C_{k}|x_{i}) = \\frac{e^{H_{T}^{C_{k}}(x_{i})}}{\\sum_{m=1}^{N}e^{H_{T}^{C_{m}}(x_{i})}}\n",
    "$$\n",
    "unde probabilitatea $P(C_{k}|x_{i})$ cea mai mare indică prezicerea clasei $k$. Astfel, problema se reduce la a minimiza divergența Kullback–Leibler între distribuția de probabilități prezisă și cea reală. Pentru a realiza acest lucru, vom antrena câte un nou clasificator pe valorile reziduale ale ipotezei pentru fiecare clasă în parte.  Adică, vom obține câte un nou $h_{t}^{C_{k}}$, realizând apoi ansamblul definit de $H_{T}^{C_{k}} = H_{T-1}^{C_{k}} + h_{t}^{C_{k}}$ unde $k={1,...,N}$\n",
    "<a id='cerinte-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cerințe (Partea I) [8p]\n",
    "Se vor antrena pentru a fi comparați următorii algoritmi pentru **clasificare**:\n",
    "* Arbore de decizie\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "* Gradient Boosting\n",
    "\n",
    "Urmăriți scheletul de cod de mai jos și completați cu cod acolo unde este indicat. Este recomandat să utilizați biblioteca **scikit-learn** unde regăsiți toate modelele descrise mai sus, împreună cu metode de antrenare și testare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 0 - import de modele din sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vom utiliza un set de date 2D generat pe baza unei distribuții Gausiene\n",
    "X, y = make_gaussian_quantiles(n_samples=10000, n_features=2, n_classes=3, random_state=19)\n",
    "\n",
    "# TODO 1 - Realizați împărțirea setului de date în train și test\n",
    "split_percent = 0.8\n",
    "X_train, X_test = None,None\n",
    "y_train, y_test = None,None\n",
    "\n",
    "# TODO 2 - Definiți modelele și dați valori parametrilor (importanți) ai acestora. \n",
    "# De exemplu, numărul de clasificatori slabi (n_estimators, dacă este cazul) pentru fiecare dintre modele.\n",
    "decision_tree = None\n",
    "random_forest = None\n",
    "adaboost = None\n",
    "gradient_boosting = None\n",
    "\n",
    "# TODO 3 - Antrenați modelele pe seturile de date\n",
    "\n",
    "# TODO 4 - Determinați valorile prezise de modele pe datele de test și calculați **erorile** (utilizați funcția \n",
    "# accuracy_score. Pentru fiecare model veți calcula eroarea la fiecare iterație de antrenare a acestuia, adică\n",
    "# de fiecare dată când este crescut numărul clasificatori slabi din ansamblu (unde este cazul). Pentru a realiza\n",
    "# acest lucru consultați funcția staged_predict() din documentația modelelor.\n",
    "#\n",
    "# Atenție! Pentru o vizualizare mai bună a rezultatelor, întrucât arborele de decizie și random forest întorc o \n",
    "# singură valoare, pentru a nu afișa un singur punct vom copia această valoare într-un vector de lungimea cea mai\n",
    "# mare corespondentă numărului maxim de iterații de antrenare realizate de modelele cu ansamblu.\n",
    "adaboost_errors = []\n",
    "gradient_boosting_errors = []\n",
    "decision_tree_error = -1.\n",
    "random_forest_error = -1.\n",
    "\n",
    "N = max(len(adaboost_errors),len(gradient_boosting_errors))\n",
    "random_forest_errors = [random_forest_error] * N\n",
    "decision_tree_errors = [decision_tree_error] * N\n",
    "\n",
    "# TODO 5 - Variați numărul de estimatori (și alți parametri relevanți) ai metodelor cu ansamblu și \n",
    "# explicați rezultatele\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian divided into three quantiles\", fontsize='small')\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, len(random_forest_errors) + 1),\n",
    "         random_forest_errors, c='black',\n",
    "         linestyle='dashed', label='Random Forest')\n",
    "plt.plot(range(1, len(decision_tree_errors) + 1),\n",
    "         decision_tree_errors, c='black',\n",
    "         linestyle='-.', label='Decision Tree')\n",
    "plt.plot(range(1, len(adaboost_errors) + 1),\n",
    "         adaboost_errors, c='blue',\n",
    "         linestyle='-', label='AdaBoost')\n",
    "plt.plot(range(1, len(gradient_boosting_errors) + 1),\n",
    "         gradient_boosting_errors, c='red',\n",
    "         linestyle='-', label='Gradient Boosting')\n",
    "plt.legend()\n",
    "plt.ylim(0.02, 0.6)\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Number of Trees (for ensembles)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Noțiuni teoretice despre Cross-Validation\n",
    "După cum știm din laboratoarele trecute, seturile de date trebuie împărțite (aleator) în (minim) două sub-seturi de date: unul pentru _antrenare_ și unul pentru _testare_. Motivul acestei împărțiri este că ne dorim să aflăm comportamentul modelului antrenat pe un set de date necunoscut (neutilizat la antrenare) pentru a estima care este capacitatea acestuia de a generaliza (cum se va comporta pe date complet noi odată ce va fi pus în producție). \n",
    "\n",
    "În cazul în care modelul clasifică foarte bine setul de antrenare dar greșește foarte mult pe setul de test, avem o indicație clară că modelul suferă de „overfitting”. Mai mult, pentru unii algoritmi este nevoie să căutăm anumite valori de hiper-parametri. De exemplu, câți clasificatori sunt suficienți pentru o metodă tip ansamblu pentru a obține performanțe maxime ;). Dacă testăm doar cu cele două sub-seturi (antrenare și test) riscăm să ajungem la overfitting întrucât nu vom fi siguri dacă pe date noi am făcut cea mai bună alegere de hiper-parametri. Soluția o reprezintă împărțirea setului inițial în 3 seturi de data: antrenare, _validare_ și testare. Modelele vor învăța din setul de antrenare, vom căuta valorile bune pentru hiper-parametri folosinde setul de validare și vom testa capacitatea de generalizare folosind setul de testare.\n",
    "\n",
    "Cu toate acestea, dacă am dori să ne asigurăm că fiecare set de date este suficient de mare pentru a fi relevant ar însemna să micșorăm semnificativ setul de antrenare (echilibrând ca dimensiune pe cele de validare și testare). Mai mult, este posibil ca însăși repartizarea aleatoare a datelor în cele 3 sub-seturi să producă dezechilibre, unele clase ajungând să nu fie corect reprezentate din punct de vedere al frecvenței (sunt puține exemple într-un sub-set relativ la câte exemple erau din clasa respectivă în setul inițial).\n",
    "\n",
    "O metodă care ameliorează aceste probleme este **cross-validation**. Practic, setul de date se va împărți întâi în două. O parte o păstrăm pentru testarea finală, urmând ca cealaltă parte (de obicei, mai mare) să fie folosită pentru antrenare și validare. Fracția de date pentru antrenare și validare este apoi separată în **k-folds** (în $k$ sub-seturi). Pe rând, $k-1$ dintre ele sunt folosite pentru antrenarea modelului în timp ce al $k$-lea este utilizat pentru validare. Procesul va întoarce valoarea medie a metricii dorite pentru fiecare astfel de pas de antrenare.\n",
    "\n",
    "![Cross-Validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png \"Cross-Validation Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cerințe (Partea II) [2p]\n",
    "Modificați implementarea de la [Partea I](#cerinte-1) pentru a utiliza **k-fold cross-validation**. Încercați cu diferite valori ale lui $k$. Este recomandată utilizarea funcției _cross_val_score_.\n",
    "\n",
    "Atenție! Este de ajuns să calculați scorul de cross-validation pe modelul „final” în cazul metodelor cu ansamblu (nu e nevoie de staged_predict). Prin urmare, veți compara doar valori de _acuratețe_ între modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vom utiliza un set de date 2D generat pe baza unei distribuții Gausiene\n",
    "X, y = make_gaussian_quantiles(n_samples=10000, n_features=2, n_classes=3, random_state=19)\n",
    "\n",
    "# TODO 6 - Definiți modelele pe care doriți să faceți cross-validation (cu hiper-parametri doriți) \n",
    "decision_tree = None\n",
    "random_forest = None\n",
    "adaboost = None\n",
    "gradient_boosting = None\n",
    "\n",
    "# TODO 7 - Aplicați cross-validation pe fiecare din modelele definite mai sus. Afișați media (mean) si deviația standard (std) a scorurilor\n",
    "# pentru fiecare valoare k aleasă\n",
    "\n",
    "# TODO 8 - Variați numărul de atribute (feautures) și numărul de clase din setul de date și explicați rezultatele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referințe\n",
    "[1] Freund, Yoav, and Robert E. Schapire. \"A desicion-theoretic generalization of on-line learning and an application to boosting.\" In European conference on computational learning theory, pp. 23-37. Springer, Berlin, Heidelberg, 1995.\n",
    "\n",
    "[2] Freund, Yoav, and Robert E. Schapire. \"Experiments with a new boosting algorithm.\" In icml, vol. 96, pp. 148-156. 1996."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resurse\n",
    "\n",
    "* http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
